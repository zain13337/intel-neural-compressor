<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Training and Inference (Evaluation) &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../versions.html">1.14.2▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Distributed Training and Inference (Evaluation)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/distributed.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="distributed-training-and-inference-evaluation">
<h1>Distributed Training and Inference (Evaluation)<a class="headerlink" href="#distributed-training-and-inference-evaluation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Neural Compressor uses <a class="reference external" href="https://github.com/horovod/horovod">horovod</a> for distributed training.</p>
</div>
<div class="section" id="horovod-installation">
<h2>horovod installation<a class="headerlink" href="#horovod-installation" title="Permalink to this headline">¶</a></h2>
<p>Please check horovod installation documentation and use following commands to install horovod:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">horovod</span>
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h2>Distributed training and inference (evaluation)<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Distributed training and inference are supported in PyTorch and TensorFlow currently. (i.e. PyTorch currently supports distributed QAT and PTQ. TensorFlow currently supports distributed PTQ and Keras-backend Pruning). To enable distributed training or inference, the steps are:</p>
<ol class="simple">
<li><p>Setting up distributed training or inference scripts. We have 2 options here:</p>
<ul class="simple">
<li><p>Option 1: Enable distributed training or inference with pure yaml configuration. In this case, Neural Compressor builtin training function is used.</p></li>
<li><p>Option 2: Pass the user defined training function to Neural Compressor. In this case, please follow the horovod documentation and below example to know how to write such training function with horovod on different frameworks.</p></li>
</ul>
</li>
<li><p>use horovodrun to execute your program.</p></li>
</ol>
<div class="section" id="option-1-pure-yaml-configuration">
<h3>Option 1: pure yaml configuration<a class="headerlink" href="#option-1-pure-yaml-configuration" title="Permalink to this headline">¶</a></h3>
<p>To enable distributed training in Neural Compressor, user only need to add a field: <code class="docutils literal notranslate"><span class="pre">Distributed:</span> <span class="pre">True</span></code> in dataloader configuration:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataloader</span><span class="p">:</span>
  <span class="n">batch_size</span><span class="p">:</span> <span class="mi">256</span>
  <span class="n">distributed</span><span class="p">:</span> <span class="kc">True</span>
  <span class="n">dataset</span><span class="p">:</span>
    <span class="n">ImageFolder</span><span class="p">:</span>
      <span class="n">root</span><span class="p">:</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">dataset</span>
</pre></div>
</div>
<p>In user’s code, pass the yaml file to Neural Compressor components, in which it constructs the real dataloader for the distributed training or inference. The example codes are as following. (TensorFlow 1.x additionally needs to enable Eager execution):</p>
<p>Do quantization based on distributed training/inference</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># import tensorflow as tf                      (Only TensorFlow 1.x needs)</span>
<span class="c1"># tf.compat.v1.enable_eager_execution()        (Only TensorFlow 1.x needs)</span>
<span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="k">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="s2">&quot;yaml_file_path&quot;</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>                          <span class="c1"># q_model -&gt; quantized low-precision model </span>
</pre></div>
</div>
<p>Only do model accuracy evaluation based on distributed inference</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># import tensorflow as tf                      (Only TensorFlow 1.x needs)</span>
<span class="c1"># tf.compat.v1.enable_eager_execution()        (Only TensorFlow 1.x needs)</span>
<span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="k">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="s2">&quot;yaml_file_path&quot;</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span> 
<span class="n">quantizer</span><span class="o">.</span><span class="n">pre_process</span><span class="p">()</span>                        <span class="c1"># If you simply want to do model evaluation with no need quantization, you should preprocess the quantizer before evaluation.</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">evaluation_result</span>  <span class="c1"># result -&gt; (accuracy, evaluation_time_cost)</span>
</pre></div>
</div>
</div>
<div class="section" id="option2-user-defined-training-function">
<h3>Option2: user defined training function<a class="headerlink" href="#option2-user-defined-training-function" title="Permalink to this headline">¶</a></h3>
<p>Neural Compressor supports User defined PyTorch training function for distributed training which requires user to modify training script following horovod requirements. We provide a MNIST example to show how to do that and following are the steps for PyTorch.</p>
<ul class="simple">
<li><p>Partition dataset via DistributedSampler:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">rank</span><span class="o">=</span><span class="n">hvd</span><span class="o">.</span><span class="n">rank</span><span class="p">())</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span><span class="o">**</span><span class="n">train_kwargs</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Wrap Optimizer:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adadelta</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">hvd</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">named_parameters</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Broadcast parameters to processes from rank 0:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_optimizer_state</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Prepare training function:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">train_loader</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train Epoch: </span><span class="si">{}</span><span class="s1"> [</span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1"> (</span><span class="si">{:.0f}</span><span class="s1">%)]</span><span class="se">\t</span><span class="s1">Loss: </span><span class="si">{:.6f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">sampler</span><span class="p">),</span>
                    <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">sampler</span><span class="p">),</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
                <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">dry_run</span><span class="p">:</span>
                    <span class="k">break</span>

<span class="k">def</span> <span class="nf">train_func</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">train</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Use user defined training function in Neural Compressor:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="k">import</span> <span class="n">Component</span><span class="p">,</span> <span class="n">common</span>
<span class="n">component</span> <span class="o">=</span> <span class="n">Component</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">)</span>
<span class="n">component</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">component</span><span class="o">.</span><span class="n">train_func</span> <span class="o">=</span> <span class="n">train_func</span>
<span class="n">component</span><span class="o">.</span><span class="n">eval_func</span> <span class="o">=</span> <span class="n">test_func</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">component</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="horovodrun">
<h3>horovodrun<a class="headerlink" href="#horovodrun" title="Permalink to this headline">¶</a></h3>
<p>User needs to use horovodrun to execute distributed training. For more usage, please refer to <a class="reference external" href="https://horovod.readthedocs.io/en/stable/running_include.html">horovod documentation</a>.</p>
<p>Following command specified the number of processes and hosts to do distributed training.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">horovodrun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">num_of_processes</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">H</span> <span class="o">&lt;</span><span class="n">hosts</span><span class="o">&gt;</span> <span class="n">python</span> <span class="n">example</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>For example, the following command means that two processes will be assigned to the two nodes ‘node-001’ and ‘node-002’. The two processes will execute ‘example.py’ at the same time. One process is executed on node ‘node-001’ and one process is executed on node ‘node-002’.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">horovodrun</span> <span class="o">-</span><span class="n">np</span> <span class="mi">2</span> <span class="o">-</span><span class="n">H</span> <span class="n">node</span><span class="o">-</span><span class="mi">001</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="n">node</span><span class="o">-</span><span class="mi">002</span><span class="p">:</span><span class="mi">1</span> <span class="n">python</span> <span class="n">example</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="security">
<h2>security<a class="headerlink" href="#security" title="Permalink to this headline">¶</a></h2>
<p>horovodrun requires user set up SSH on all hosts without any prompts. To do distributed training with Neural Compressor, user needs to ensure the SSH setting on all hosts.</p>
</div>
<div class="section" id="following-examples-are-supported">
<h2>Following examples are supported<a class="headerlink" href="#following-examples-are-supported" title="Permalink to this headline">¶</a></h2>
<p>PyTorch:</p>
<ul class="simple">
<li><p>PyTorch example-1: MNIST</p>
<ul>
<li><p>Please follow this README.md exactly：<a class="reference external" href="https://github.com/intel/neural-compressor/tree/566a9c1f0709a21aa8bed27c39ca2f25bd6ad783/docs/../examples/pytorch/image_recognition/mnist">MNIST</a></p></li>
</ul>
</li>
<li><p>PyTorch example-2: QAT (Quantization Aware Training)</p>
<ul>
<li><p>Please follow this README.md exactly：<a class="reference external" href="https://github.com/intel/neural-compressor/tree/566a9c1f0709a21aa8bed27c39ca2f25bd6ad783/docs/../examples/pytorch/image_recognition/torchvision_models/quantization/qat/eager/distributed">QAT</a></p></li>
</ul>
</li>
</ul>
<p>TensorFlow:</p>
<ul>
<li><p>TensorFlow example-1: ‘ResNet50 V1.0’ PTQ (Post Training Quantization) with distributed inference</p>
<ul>
<li><p>Step-1: Please cd (change directory) to the <a class="reference external" href="https://github.com/intel/neural-compressor/tree/566a9c1f0709a21aa8bed27c39ca2f25bd6ad783/docs/../examples/tensorflow/image_recognition">TensorFlow Image Recognition Example</a> and follow the readme to run PTQ, ensure that PTQ of ‘ResNet50 V1.0’ can be successfully executed.</p></li>
<li><p>Step-2: We only need to modify the <a class="reference external" href="https://github.com/intel/neural-compressor/blob/566a9c1f0709a21aa8bed27c39ca2f25bd6ad783/docs/../examples/tensorflow/image_recognition/tensorflow_models/quantization/ptq/resnet50_v1.yaml">resnet50_v1.yaml</a>, add a line ‘distributed: True’ in the ‘evaluation’ field.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># only need to modify the resnet50_v1.yaml, add a line &#39;distributed: True&#39;</span>
<span class="o">......</span>
<span class="o">......</span>
<span class="n">evaluation</span><span class="p">:</span>                                          <span class="c1"># optional. required if user doesn&#39;t provide eval_func in neural_compressor.Quantization.</span>
  <span class="n">accuracy</span><span class="p">:</span>                                          <span class="c1"># optional. required if user doesn&#39;t provide eval_func in neural_compressor.Quantization.</span>
    <span class="n">metric</span><span class="p">:</span>
      <span class="n">topk</span><span class="p">:</span> <span class="mi">1</span>                                        <span class="c1"># built-in metrics are topk, map, f1, allow user to register new metric.</span>
    <span class="n">dataloader</span><span class="p">:</span>
      <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
      <span class="n">distributed</span><span class="p">:</span> <span class="kc">True</span>                              <span class="c1"># add a line &#39;distributed: True&#39;</span>
      <span class="n">dataset</span><span class="p">:</span>
        <span class="n">ImageRecord</span><span class="p">:</span>
          <span class="n">root</span><span class="p">:</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">evaluation</span><span class="o">/</span><span class="n">dataset</span>          <span class="c1"># NOTE: modify to evaluation dataset location if needed</span>
      <span class="n">transform</span><span class="p">:</span>
        <span class="n">ResizeCropImagenet</span><span class="p">:</span> 
          <span class="n">height</span><span class="p">:</span> <span class="mi">224</span>
          <span class="n">width</span><span class="p">:</span> <span class="mi">224</span>
<span class="o">......</span>
<span class="o">......</span>
</pre></div>
</div>
</li>
<li><p>Step-3: Execute ‘main.py’ with horovodrun as following command, it will realize PTQ based on two-node tow-process distributed inference. Please replace these fields according to your actual situation: ‘your_node1_name’, ‘your_node2_name’, ‘/PATH/TO/’. (Note that if you use TensorFlow 1.x now, you need to add a line ‘tf.compat.v1.enable_eager_execution()’ into ‘main.py’ to enable Eager execution.)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">horovodrun</span> <span class="o">-</span><span class="n">np</span> <span class="mi">2</span> <span class="o">-</span><span class="n">H</span> <span class="n">your_node1_name</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="n">your_node2_name</span><span class="p">:</span><span class="mi">1</span> <span class="n">python</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">tune</span> <span class="o">--</span><span class="n">config</span><span class="o">=</span><span class="n">resnet50_v1</span><span class="o">.</span><span class="n">yaml</span> <span class="o">--</span><span class="nb">input</span><span class="o">-</span><span class="n">graph</span><span class="o">=/</span><span class="n">PATH</span><span class="o">/</span><span class="n">TO</span><span class="o">/</span><span class="n">resnet50_fp32_pretrained_model</span><span class="o">.</span><span class="n">pb</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">graph</span><span class="o">=./</span><span class="n">nc_resnet50_v1</span><span class="o">.</span><span class="n">pb</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>TensorFlow example-2: ‘resnet_v2’ pruning on Keras backend with distributed training and inference</p>
<ul class="simple">
<li><p>Please follow this README.md exactly：<a class="reference external" href="https://github.com/intel/neural-compressor/tree/566a9c1f0709a21aa8bed27c39ca2f25bd6ad783/docs/../examples/tensorflow/image_recognition/resnet_v2">Pruning</a></p></li>
</ul>
</li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>