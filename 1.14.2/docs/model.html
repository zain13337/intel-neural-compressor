<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Model &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../versions.html">1.14.2▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/model.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="model">
<h1>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h1>
<p>The Neural Compressor Model feature is used to encapsulate the behavior of model building and saving. By simply providing information such as different model formats and framework_specific_info, Neural Compressor performs optimizations and quantization on this model object and returns an Neural Compressor Model object for further model persisting or benchmarking. An Neural Compressor Model helps users to maintain necessary model information which is needed during optimization and quantization such as the input/output names, workspace path, and other model format knowledge. This helps unify the features gap brought by different model formats and frameworks.</p>
<p>Users can create, use, and save models in the following manner:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor</span> <span class="kn">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="s1">&#39;./conf.yaml&#39;</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="s1">&#39;/path/to/model&#39;</span>
<span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">q_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="framework-model-support-list">
<h2>Framework model support list<a class="headerlink" href="#framework-model-support-list" title="Permalink to this headline">¶</a></h2>
<div class="section" id="tensorflow">
<h3>TensorFlow<a class="headerlink" href="#tensorflow" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model format</th>
<th>Parameters</th>
<th>Comments</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>frozen pb</td>
<td><strong>model</strong>(str): path to frozen pb <br> <strong>framework_specific_info</strong>(dict): information about model and framework, such as input_tensor_names, input_tensor_names, workspace_path and name <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Examples</strong>: <br> <a href="../examples/tensorflow/image_recognition">../examples/tensorflow/image_recognition</a> <br> <a href="../examples/tensorflow/oob_models">../examples/tensorflow/oob_models</a> <br> <strong>Save format</strong>: <br> frozen pb</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is the path of model, like ./path/to/frozen.pb</strong></td>
</tr>
<tr>
<td>Graph object</td>
<td><strong>model</strong>(tf.compat.v1.Graph): tf.compat.v1.Graph object  <br> <strong>framework_specific_info</strong>(dict): information about model and framework, such as input_tensor_names, input_tensor_names, workspace_path and name <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Examples</strong>: <br> <a href="../examples/tensorflow/style_transfer">../examples/tensorflow/style_transfer</a> <br> <a href="../examples/tensorflow/recommendation/wide_deep_large_ds">../examples/tensorflow/recommendation/wide_deep_large_ds</a> <br> <strong>Save format</strong>: <br> frozen pb</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is the object of tf.compat.v1.Graph</strong></td>
</tr>
<tr>
<td>Graph object</td>
<td><strong>model</strong>(tf.compat.v1.GraphDef) tf.compat.v1.GraphDef object <br> <strong>framework_specific_info</strong>(dict): information about model and framework, such as input_tensor_names, input_tensor_names, workspace_path and name <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Save format</strong>: <br> frozen pb</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is the object of tf.compat.v1.GraphDef</strong></td>
</tr>
<tr>
<td>tf1.x checkpoint</td>
<td><strong>model</strong>(str): path to checkpoint <br> <strong>framework_specific_info</strong>(dict): information about model and framework, such as input_tensor_names, input_tensor_names, workspace_path and name <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Examples</strong>: <br> <a href="../examples/helloworld/tf_example4">../examples/helloworld/tf_example4</a> <br> <a href="../examples/tensorflow/object_detection">../examples/tensorflow/object_detection</a>  <br> <strong>Save format</strong>: <br> frozen pb</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is the path of model, like ./path/to/ckpt/</strong></td>
</tr>
<tr>
<td>keras.Model object</td>
<td><strong>model</strong>(tf.keras.Model): tf.keras.Model object <br> <strong>framework_specific_info</strong>(dict): information about model and framework, such as input_tensor_names, input_tensor_names, workspace_path and name <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Save format</strong>: <br> keras saved model</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is the object of tf.keras.Model</strong></td>
</tr>
<tr>
<td>keras saved model</td>
<td><strong>model</strong>(str): path to keras saved model <br> <strong>framework_specific_info</strong>(dict): information about model and framework, such as input_tensor_names, input_tensor_names, workspace_path and name <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Examples</strong>: <br> <a href="../examples/helloworld/tf_example2">../examples/helloworld/tf_example2</a> <br> <strong>Save format</strong>: <br> keras saved model</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is the path of model, like ./path/to/saved_model/</strong></td>
</tr>
<tr>
<td>tf2.x saved model</td>
<td><strong>model</strong>(str): path to saved model <br> <strong>framework_specific_info</strong>(dict): information about model and framework, such as input_tensor_names, input_tensor_names, workspace_path and name <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Save format</strong>: <br> saved model</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is the path of model, like ./path/to/saved_model/</strong></td>
</tr>
<tr>
<td>tf2.x h5 format model</td>
<td></td>
<td>TBD</td>
<td></td>
</tr>
<tr>
<td>slim checkpoint</td>
<td><strong>model</strong>(str): path to slim checkpoint <br> <strong>framework_specific_info</strong>(dict): information about model and framework, such as input_tensor_names, input_tensor_names, workspace_path and name <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Examples</strong>: <br> <a href="../examples/helloworld/tf_example3">../examples/helloworld/tf_example3</a> <br> <strong>Save format</strong>: <br> frozen pb</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is thepath of model, like ./path/to/model.ckpt</strong></td>
</tr>
<tr>
<td>tf1.x saved model</td>
<td><strong>model</strong>(str): path to saved model, <strong>framework_specific_info</strong>(dict): information about model and framework, such as input_tensor_names, input_tensor_names, workspace_path and name <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Save format</strong>: <br> saved model</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is the path of model, like ./path/to/saved_model/</strong></td>
</tr>
<tr>
<td>tf2.x checkpoint</td>
<td></td>
<td>Not support yes. As tf2.x checkpoint only has weight and does not contain any description of the computation, please use different tf2.x model for quantization</td>
<td></td>
</tr>
</tbody>
</table><p>The following methods can be used in the TensorFlow model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">graph_def</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">graph_def</span>
<span class="n">input_tensor_names</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">input_tensor_names</span>
<span class="n">model</span><span class="o">.</span><span class="n">input_tensor_names</span> <span class="o">=</span> <span class="n">input_tensor_names</span>
<span class="n">output_tensor_names</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">output_tensor_names</span>
<span class="n">model</span><span class="o">.</span><span class="n">output_tensor_names</span> <span class="o">=</span> <span class="n">output_tensor_names</span>
<span class="n">input_node_names</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">input_node_names</span>
<span class="n">output_node_names</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">output_node_names</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">input_tensor</span>
<span class="n">output_tensor</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">output_tensor</span>
</pre></div>
</div>
</div>
<div class="section" id="mxnet">
<h3>MXNet<a class="headerlink" href="#mxnet" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model format</th>
<th>Parameters</th>
<th>Comments</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>mxnet.gluon.HybridBlock</td>
<td><strong>model</strong>(mxnet.gluon.HybridBlock): mxnet.gluon.HybridBlock object <br> <strong>framework_specific_info</strong>(dict): information about model and framework <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Save format</strong>: <br> save_path.json</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is mxnet.gluon.HybridBlock object</strong></td>
</tr>
<tr>
<td>mxnet.symbol.Symbol</td>
<td><strong>model</strong>(tuple): tuple of symbol, arg_params, aux_params <br> <strong>framework_specific_info</strong>(dict): information about model and framework <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Save format</strong>: <br> save_path-symbol.json and save_path-0000.params</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is the tuple of symbol, arg_params, aux_params</strong></td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Get symbol, arg_params, aux_params from symbol and param files.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mxnet</span> <span class="kn">as</span> <span class="nn">mx</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">nd</span>

<span class="n">symbol</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">sym</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">symbol_file_path</span><span class="p">)</span>
<span class="n">save_dict</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">param_file_path</span><span class="p">)</span>
<span class="n">arg_params</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">aux_params</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">save_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">tp</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tp</span> <span class="o">==</span> <span class="s1">&#39;arg&#39;</span><span class="p">:</span>
        <span class="n">arg_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="k">if</span> <span class="n">tp</span> <span class="o">==</span> <span class="s1">&#39;aux&#39;</span><span class="p">:</span>
        <span class="n">aux_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch">
<h3>PyTorch<a class="headerlink" href="#pytorch" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model format</th>
<th>Parameters</th>
<th>Comments</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.model</td>
<td><strong>model</strong>(torch.nn.model): torch.nn.model object <br> <strong>framework_specific_info</strong>(dict): information about model and framework <br> <strong>kwargs</strong>(dict): other required parameters</td>
<td><strong>Save format</strong>: <br> Without Intel PyTorch Extension(IPEX): /save_path/best_configure.yaml and /save_path/best_model_weights.pt <br> With IPEX: /save_path/best_configure.json</td>
<td>from neural_compressor.experimental import Quantization, common <br> quantizer = Quantization(args.config) <br> quantizer.model = model <br> q_model = quantizer.fit() <br> <strong>model is torch.nn.model object</strong></td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Loading model:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Without IPEX</span>
<span class="kn">from</span> <span class="nn">neural_compressor.utils.pytorch</span> <span class="kn">import</span> <span class="n">load</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">Path</span><span class="p">)),</span> <span class="n">model</span><span class="p">)</span> <span class="c1"># model is a fp32 model</span>

<span class="c1"># With IPEX</span>
<span class="kn">import</span> <span class="nn">intel_pytorch_extension</span> <span class="kn">as</span> <span class="nn">ipex</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span> <span class="c1"># model is a fp32 model</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">new_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">new_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">))</span>
<span class="n">ipex_config_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">tuned_checkpoint</span><span class="p">),</span>
                                <span class="s2">&quot;best_configure.json&quot;</span><span class="p">)</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AmpConf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">configure_file</span><span class="o">=</span><span class="n">ipex_config_path</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AutoMixPrecision</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span> <span class="n">running_mode</span><span class="o">=</span><span class="s1">&#39;inference&#39;</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">new_model</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>