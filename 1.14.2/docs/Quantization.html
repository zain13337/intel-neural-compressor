<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quantization &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PTQ" href="PTQ.html" />
    <link rel="prev" title="Metrics" href="metric.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../versions.html">1.14.2▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../README.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="doclist.html">Developer Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="doclist.html#get-started">Get Started</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="doclist.html#deep-dive">Deep Dive</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#quantization-support-matrix">Quantization Support Matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="#examples-of-quantization">Examples of Quantization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="PTQ.html">PTQ</a></li>
<li class="toctree-l3"><a class="reference internal" href="QAT.html">Quantization-aware Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="dynamic_quantization.html">Dynamic Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="pruning.html">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="benchmark.html">Benchmarking</a></li>
<li class="toctree-l3"><a class="reference internal" href="mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="graph_optimization.html">Graph Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_conversion.html">Model Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensorboard.html">TensorBoard</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="doclist.html#advanced-topics">Advanced Topics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="doclist.html">Developer Documentation</a> &raquo;</li>
      <li>Quantization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/Quantization.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Permalink to this headline">¶</a></h1>
<p>Quantization is a widely-used model compression technique that can reduce model size while also improving inference and training latency.</br>
The full precision data converts to low-precision, there is little degradation in model accuracy, but the inference performance of quantized model can gain higher performance by saving the memory bandwidth and accelerating computations with low precision instructions. Intel provided several lower precision instructions (ex: 8-bit or 16-bit multipliers), both training and inference can get benefits from them.
Refer to the Intel article on <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/articles/lower-numerical-precision-deep-learning-inference-and-training.html">lower numerical precision inference and training in deep learning</a>.</p>
<div class="section" id="quantization-support-matrix">
<h2>Quantization Support Matrix<a class="headerlink" href="#quantization-support-matrix" title="Permalink to this headline">¶</a></h2>
<p>Quantization methods include the following three types:</p>
<table class="center">
    <thead>
        <tr>
            <th>Types</th>
            <th>Quantization</th>
            <th>Dataset Requirements</th>
            <th>Framework</th>
            <th>Backend</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="3" align="center">Post-Training Static Quantization (PTQ)</td>
            <td rowspan="3" align="center">weights and activations</td>
            <td rowspan="3" align="center">calibration</td>
            <td align="center">PyTorch</td>
            <td align="center"><a href="https://pytorch.org/docs/stable/quantization.html#eager-mode-quantization">PyTorch Eager</a>/<a href="https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization">PyTorch FX</a>/<a href="https://github.com/intel/intel-extension-for-pytorch">IPEX</a></td>
        </tr>
        <tr>
            <td align="center">TensorFlow</td>
            <td align="center"><a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>/<a href="https://github.com/Intel-tensorflow/tensorflow">Intel TensorFlow</a></td>
        </tr>
        <tr>
            <td align="center">ONNX Runtime</td>
            <td align="center"><a href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/quantize.py">QLinearops/QDQ</a></td>
        </tr>
        <tr>
            <td rowspan="2" align="center">Post-Training Dynamic Quantization</td>
            <td rowspan="2" align="center">weights</td>
            <td rowspan="2" align="center">none</td>
            <td align="center">PyTorch</td>
            <td align="center"><a href="https://pytorch.org/docs/stable/quantization.html#eager-mode-quantization">PyTorch eager mode</a>/<a href="https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization">PyTorch fx mode</a>/<a href="https://github.com/intel/intel-extension-for-pytorch">IPEX</a></td>
        </tr>
        <tr>
            <td align="center">ONNX Runtime</td>
            <td align="center"><a href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/quantize.py">QIntegerops</a></td>
        </tr>  
        <tr>
            <td rowspan="2" align="center">Quantization-aware Training (QAT)</td>
            <td rowspan="2" align="center">weights and activations</td>
            <td rowspan="2" align="center">fine-tuning</td>
            <td align="center">PyTorch</td>
            <td align="center"><a href="https://pytorch.org/docs/stable/quantization.html#eager-mode-quantization">PyTorch eager mode</a>/<a href="https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization">PyTorch fx mode</a>/<a href="https://github.com/intel/intel-extension-for-pytorch">IPEX</a></td>
        </tr>
        <tr>
            <td align="center">TensorFlow</td>
            <td align="center"><a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>/<a href="https://github.com/Intel-tensorflow/tensorflow">Intel TensorFlow</a></td>
        </tr>
    </tbody>
</table>
<br>
<br><div class="section" id="post-training-static-quantization-performs-quantization-on-already-trained-models-it-requires-an-additional-pass-over-the-dataset-to-work-only-activations-do-calibration">
<h3><a class="reference internal" href="PTQ.html"><span class="doc">Post-Training Static Quantization</span></a> performs quantization on already trained models, it requires an additional pass over the dataset to work, only activations do calibration.<a class="headerlink" href="#post-training-static-quantization-performs-quantization-on-already-trained-models-it-requires-an-additional-pass-over-the-dataset-to-work-only-activations-do-calibration" title="Permalink to this headline">¶</a></h3>
<img src="../docs/imgs/PTQ.png" width=256 height=129 alt="PTQ">
<br></div>
<div class="section" id="post-training-dynamic-quantization-simply-multiplies-input-values-by-a-scaling-factor-then-rounds-the-result-to-the-nearest-it-determines-the-scale-factor-for-activations-dynamically-based-on-the-data-range-observed-at-runtime-weights-are-quantized-ahead-of-time-but-the-activations-are-dynamically-quantized-during-inference">
<h3><a class="reference internal" href="dynamic_quantization.html"><span class="doc">Post-Training Dynamic Quantization</span></a> simply multiplies input values by a scaling factor, then rounds the result to the nearest, it determines the scale factor for activations dynamically based on the data range observed at runtime. Weights are quantized ahead of time but the activations are dynamically quantized during inference.<a class="headerlink" href="#post-training-dynamic-quantization-simply-multiplies-input-values-by-a-scaling-factor-then-rounds-the-result-to-the-nearest-it-determines-the-scale-factor-for-activations-dynamically-based-on-the-data-range-observed-at-runtime-weights-are-quantized-ahead-of-time-but-the-activations-are-dynamically-quantized-during-inference" title="Permalink to this headline">¶</a></h3>
<img src="../docs/imgs/dynamic_quantization.png" width=270 height=124 alt="Dynamic Quantization">
<br></div>
<div class="section" id="quantization-aware-training-qat-quantizes-models-during-training-and-typically-provides-higher-accuracy-comparing-with-post-training-quantization-but-qat-may-require-additional-hyper-parameter-tuning-and-it-may-take-more-time-to-deployment">
<h3><a class="reference internal" href="QAT.html"><span class="doc">Quantization-aware Training (QAT)</span></a> quantizes models during training and typically provides higher accuracy comparing with post-training quantization, but QAT may require additional hyper-parameter tuning and it may take more time to deployment.<a class="headerlink" href="#quantization-aware-training-qat-quantizes-models-during-training-and-typically-provides-higher-accuracy-comparing-with-post-training-quantization-but-qat-may-require-additional-hyper-parameter-tuning-and-it-may-take-more-time-to-deployment" title="Permalink to this headline">¶</a></h3>
<img src="../docs/imgs/QAT.png" width=244 height=147 alt="QAT"></div>
</div>
<div class="section" id="examples-of-quantization">
<h2>Examples of Quantization<a class="headerlink" href="#examples-of-quantization" title="Permalink to this headline">¶</a></h2>
<p>For Quantization related examples, please refer to <a class="reference external" href="/examples/README">Quantization examples</a></p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="metric.html" class="btn btn-neutral float-left" title="Metrics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="PTQ.html" class="btn btn-neutral float-right" title="PTQ" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>