<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>FX &mdash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../versions.html">1.14.2▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>FX</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/FX.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="fx">
<h1>FX<a class="headerlink" href="#fx" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>FX is a PyTorch toolkit for developers to use to transform nn.Module instance. With converted torch.fx.GraphModule, we can automatically insert quant/dequant operation within PyTorch.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.fx.symbolic_trace()</span></code></p>
<p>Fake values, called Proxies, will be fed into model and record all operations in it. Then, we can get torch.fx.GraphModule from torch.nn.Module.</p>
</li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h2>
<p><strong>Note:</strong> Make sure to replace backend with <code class="docutils literal notranslate"><span class="pre">pytorch_fx</span></code> in <strong>conf.yaml</strong>.</p>
<p>Then, you can quantize model as usual with neural_compressor without editing the source code of the model, as shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">tune</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="k">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">(</span><span class="s2">&quot;./conf.yaml&quot;</span><span class="p">)</span>
    <span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="n">q_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="n">q_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">tuned_checkpoint</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="section" id="note">
<h2>Note<a class="headerlink" href="#note" title="Permalink to this headline">¶</a></h2>
<p>Right now, we support auto quantization method and can avoid below common problem.<br />For users, you will see log output below if you model failed on symbolic trace method.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">INFO</span><span class="p">]</span> <span class="n">Fx</span> <span class="n">trace</span> <span class="n">of</span> <span class="n">the</span> <span class="n">entire</span> <span class="n">model</span> <span class="n">failed</span><span class="o">.</span> <span class="n">We</span> <span class="n">will</span> <span class="n">conduct</span> <span class="n">auto</span> <span class="n">quantization</span>
</pre></div>
</div>
<div class="section" id="details">
<h3>Details<a class="headerlink" href="#details" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We combine GraphModule from symbolic_trace and imperative control flow. Therefore, the INT8 model consists of lots of GraphModules.</p></li>
</ul>
</div>
</div>
<hr class="docutils" />
<div class="section" id="common-problem">
<h2>Common Problem<a class="headerlink" href="#common-problem" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dynamic-quantization">
<h3><em>Dynamic Quantization</em><a class="headerlink" href="#dynamic-quantization" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>PyTorch Version: 1.9 or higher</p>
<p>You can use pytorch backend for dynamic quantization, there is no difference between pytorch and pytorch_fx. we don’t need to trace model because we don’t need to modify the source code of the model.</p>
</li>
</ul>
</div>
<div class="section" id="static-quantization-quantization-aware-training">
<h3><em>Static Quantization</em> &amp; <em>Quantization Aware Training</em><a class="headerlink" href="#static-quantization-quantization-aware-training" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>PyTorch Version: 1.8 or higher</p>
<p>As symbolic trace cannot handle dynamic control, tensor iteration and so on, we might meet trace failure sometimes. In order to quantize the model successfully, we suggest two approaches to preprocess the model here.</p>
<ol>
<li><p>Non_traceable_module_class/name</p>
<p>Select module classes or names that cannot be traced by proxy object, and pass them into neural_compressor: <code class="docutils literal notranslate"><span class="pre">common.Model</span></code> as a dict.</p>
<p>These non_traceable modules will be considered as a  called function. If there is any nn.Conv2D in modules, it won’t be converted into quantized::Conv2D.</p>
<p><strong>For example:</strong> example/pytorch/fx/object_detection/maskrcnn/pytorch/tools/test_net.py</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prepare_custom_config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;non_traceable_module_class&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="n">AnchorGenerator</span><span class="p">,</span> <span class="n">RPNPostProcessor</span><span class="p">,</span> <span class="n">Pooler</span><span class="p">,</span> <span class="n">PostProcessor</span><span class="p">,</span> \
    <span class="n">MaskRCNNFPNFeatureExtractor</span><span class="p">,</span> <span class="n">MaskPostProcessor</span><span class="p">,</span> <span class="n">FPN</span><span class="p">,</span> <span class="n">RPNHead</span>
    <span class="p">]}</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> \
    <span class="o">**</span><span class="p">{</span><span class="s1">&#39;prepare_custom_config_dict&#39;</span><span class="p">:</span> <span class="n">prepare_custom_config_dict</span><span class="p">})</span>
</pre></div>
</div>
</li>
<li><p>Decorator: &#64;torch.fx.wrap</p>
<p>If untraceable part is not a part of a module, like a global function called, or you want to move untraceable part out of model to keep other parts get quantized, you should try to use Decorator <code class="docutils literal notranslate"><span class="pre">&#64;torch.fx.wrap</span></code>. The wrapped function must be in the global, not in the class.</p>
<p><strong>For example:</strong> examples/pytorch/fx/object_detection/ssd_resnet34/ptq/python/models/ssd_r34.py</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>@torch.fx.wrap
def bboxes_labels_scores(bboxes, probs, criteria = 0.45, max_output=200):
    boxes = []; labels=[]; scores=[]
    for bbox, prob in zip(bboxes.split(1, 0), probs.split(1, 0)):
        bbox = bbox.squeeze(0)
        prob = prob.squeeze(0)
        dbox,dlabel,dscore=decode_single(bbox, prob, criteria, max_output)
        boxes.append(dbox)
        labels.append(dlabel)
        scores.append(dscore)
    return [boxes,labels,scores]
    ```

</pre></div>
</div>
</li>
</ol>
</li>
</ul>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>