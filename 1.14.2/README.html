<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Intel® Neural Compressor .htmlash; Intel® Neural Compressor  documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Examples" href="docs/examples_readme.html" />
    <link rel="prev" title="Intel® Neural Compressor Documentation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../versions.html">1.14.2▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Intel® Neural Compressor</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-on-linux">Install on Linux</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quantization-with-python-api">Quantization with Python API</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantization-with-jupyterlab-extension">Quantization with JupyterLab Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantization-with-gui">Quantization with GUI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#system-requirements">System Requirements</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#validated-hardware-environment">Validated Hardware Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#intel-neural-compressor-supports-cpus-based-on-intel-64-architecture-or-compatible-processors">Intel® Neural Compressor supports CPUs based on Intel 64 architecture or compatible processors:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel-neural-compressor-supports-gpus-built-on-intel-s-xe-architecture">Intel® Neural Compressor supports GPUs built on Intel’s Xe architecture:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel-neural-compressor-quantized-models-are-scalable-for-broad-devices">Intel® Neural Compressor quantized models are scalable for broad devices:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#validated-software-environment">Validated Software Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#validated-models">Validated Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#selected-publications">Selected Publications</a></li>
<li class="toctree-l2"><a class="reference internal" href="#additional-content">Additional Content</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hiring">Hiring</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="docs/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="docs/doclist.html">Developer Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="docs/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="docs/contributions.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="docs/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Intel® Neural Compressor</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/README.html.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div align="center"><div class="section" id="intel-neural-compressor">
<h1>Intel® Neural Compressor<a class="headerlink" href="#intel-neural-compressor" title="Permalink to this headline">¶</a></h1>
<p><h3> An open-source Python library supporting popular model compression techniques on all mainstream deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet)</h3></p>
<p><a class="reference external" href="https://github.com/intel/neural-compressor"><img alt="python" src="https://img.shields.io/badge/python-3.7%2B-blue" /></a>
<a class="reference external" href="https://github.com/intel/neural-compressor/releases"><img alt="version" src="https://img.shields.io/badge/release-1.14-green" /></a>
<a class="reference external" href="https://github.com/intel/neural-compressor/blob/master/LICENSE"><img alt="license" src="https://img.shields.io/badge/license-Apache%202-blue" /></a>
<a class="reference external" href="https://github.com/intel/neural-compressor"><img alt="coverage" src="https://img.shields.io/badge/coverage-90%25-green" /></a>
<a class="reference external" href="https://pepy.tech/project/neural-compressor"><img alt="Downloads" src="https://static.pepy.tech/personalized-badge/neural-compressor?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=green&amp;left_text=downloads" /></a></p>
</div><hr class="docutils" />
<div align="left"><p>Intel® Neural Compressor, formerly known as Intel® Low Precision Optimization Tool, is an open-source Python library that runs on Intel CPUs and GPUs, which delivers unified interfaces across multiple deep-learning frameworks for popular network compression technologies such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help the user quickly find out the best quantized model. It also implements different weight-pruning algorithms to generate a pruned model with predefined sparsity goal. It also supports knowledge distillation to distill the knowledge from the teacher model to the student model.
Intel® Neural Compressor is a critical AI software component in the <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html">Intel® oneAPI AI Analytics Toolkit</a>.</p>
<p><strong>Visit the Intel® Neural Compressor online document website at: <a class="reference external" href="https://intel.github.io/neural-compressor">https://intel.github.io/neural-compressor</a>.</strong></p>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h3>
<p>Python version: 3.7, 3.8, 3.9, 3.10</p>
</div>
<div class="section" id="install-on-linux">
<h3>Install on Linux<a class="headerlink" href="#install-on-linux" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Release binary install</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># install stable basic version from pip</span>
pip install neural-compressor
<span class="c1"># Or install stable full version from pip (including GUI)</span>
pip install neural-compressor-full
</pre></div>
</div>
</li>
<li><p>Nightly binary install</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/intel/neural-compressor.git
<span class="nb">cd</span> neural-compressor
pip install -r requirements.txt
<span class="c1"># install nightly basic version from pip</span>
pip install -i https://test.pypi.org/simple/ neural-compressor
<span class="c1"># Or install nightly full version from pip (including GUI)</span>
pip install -i https://test.pypi.org/simple/ neural-compressor-full
</pre></div>
</div>
</li>
</ul>
<p>More installation methods can be found at <a class="reference internal" href="docs/installation_guide.html"><span class="doc">Installation Guide</span></a>. Please check out our <a class="reference internal" href="docs/faq.html"><span class="doc">FAQ</span></a> for more details.</p>
</div>
</div>
<div class="section" id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<div class="section" id="quantization-with-python-api">
<h3>Quantization with Python API<a class="headerlink" href="#quantization-with-python-api" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># A TensorFlow Example</span>
pip install tensorflow
<span class="c1"># Prepare fp32 model</span>
wget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6/mobilenet_v1_1.0_224_frozen.pb
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">neural_compressor.experimental</span> <span class="kn">import</span> <span class="n">Quantization</span><span class="p">,</span> <span class="n">common</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantization</span><span class="p">()</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="s1">&#39;./mobilenet_v1_1.0_224_frozen.pb&#39;</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">dataset</span><span class="p">(</span><span class="s1">&#39;dummy&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="quantization-with-jupyterlab-extension">
<h3>Quantization with <a class="reference internal" href="neural_coder/extensions/neural_compressor_ext_lab/README.html"><span class="doc">JupyterLab Extension</span></a><a class="headerlink" href="#quantization-with-jupyterlab-extension" title="Permalink to this headline">¶</a></h3>
<p>Search for <code class="docutils literal notranslate"><span class="pre">jupyter-lab-neural-compressor</span></code> in the Extension Manager in JupyterLab and install with one click:</p>
<a target="_blank" href="./neural_coder/extensions/screenshots/extmanager.png">
  <img src="./neural_coder/extensions/screenshots/extmanager.png" alt="Extension" width="35%" height="35%">
</a></div>
<div class="section" id="quantization-with-gui">
<h3>Quantization with <a class="reference internal" href="docs/bench.html"><span class="doc">GUI</span></a><a class="headerlink" href="#quantization-with-gui" title="Permalink to this headline">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># An ONNX Example</span>
pip install <span class="nv">onnx</span><span class="o">==</span><span class="m">1</span>.12.0 <span class="nv">onnxruntime</span><span class="o">==</span><span class="m">1</span>.12.1 onnxruntime-extensions
<span class="c1"># Prepare fp32 model</span>
wget https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet50-v1-12.onnx
<span class="c1"># Start GUI</span>
inc_bench
</pre></div>
</div>
<a target="_blank" href="./docs/imgs/INC_GUI.gif">
  <img src="./docs/imgs/INC_GUI.gif" alt="Architecture">
</a></div>
</div>
<div class="section" id="system-requirements">
<h2>System Requirements<a class="headerlink" href="#system-requirements" title="Permalink to this headline">¶</a></h2>
<div class="section" id="validated-hardware-environment">
<h3>Validated Hardware Environment<a class="headerlink" href="#validated-hardware-environment" title="Permalink to this headline">¶</a></h3>
<div class="section" id="intel-neural-compressor-supports-cpus-based-on-intel-64-architecture-or-compatible-processors">
<h4>Intel® Neural Compressor supports CPUs based on <a class="reference external" href="https://en.wikipedia.org/wiki/X86-64">Intel 64 architecture or compatible processors</a>:<a class="headerlink" href="#intel-neural-compressor-supports-cpus-based-on-intel-64-architecture-or-compatible-processors" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Intel Xeon Scalable processor (formerly Skylake, Cascade Lake, Cooper Lake, and Icelake)</p></li>
<li><p>Future Intel Xeon Scalable processor (code name Sapphire Rapids)</p></li>
</ul>
</div>
<div class="section" id="intel-neural-compressor-supports-gpus-built-on-intel-s-xe-architecture">
<h4>Intel® Neural Compressor supports GPUs built on Intel’s Xe architecture:<a class="headerlink" href="#intel-neural-compressor-supports-gpus-built-on-intel-s-xe-architecture" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/data-center-gpu/flex-series/overview.html">Intel® Data Center GPU Flex Series</a></p></li>
</ul>
</div>
<div class="section" id="intel-neural-compressor-quantized-models-are-scalable-for-broad-devices">
<h4>Intel® Neural Compressor quantized models are scalable for broad devices:<a class="headerlink" href="#intel-neural-compressor-quantized-models-are-scalable-for-broad-devices" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Examples of ONNX INT8 model quantized by Intel® Neural Compressor verified with accuracy on INTEL/AMD/ARM CPUs and NV GPU.</p></li>
</ul>
<table class="tg">
<thead>
  <tr>
    <th class="tg-twlt" rowspan="2">INC quantized models</th>
    <th class="tg-aldk" rowspan="2">Intel ICX</th>
    <th class="tg-aldk" rowspan="2">NV A100 CUDA <br>Execution Provider</th>
    <th class="tg-aldk" rowspan="2">AMD Milan</th>
    <th class="tg-aldk" rowspan="2">ARM Graviton2</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td class="tg-cwad">ResNet50 QDQ</td>
    <td class="tg-cwad">74%</td>
    <td class="tg-cwad">74%</td>
    <td class="tg-cwad">73%</td>
    <td class="tg-cwad">74%</td>
  </tr>
  <tr>
    <td class="tg-cwad">BERT-base QDQ</td>
    <td class="tg-cwad">86%</td>
    <td class="tg-cwad">85%</td>
    <td class="tg-cwad">86%</td>
    <td class="tg-cwad">86%</td>
  </tr>
</tbody>
</table><blockquote>
<div><p><strong>Note:</strong>
More examples validated on <a class="reference external" href="https://aws.amazon.com/ec2/instance-types/">AWS</a> please check <a class="reference external" href="./docs/validated_model_list.html#Validated-ONNX-INT8-models-accuracy-on-broad-hardware">extension list</a>.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="validated-software-environment">
<h3>Validated Software Environment<a class="headerlink" href="#validated-software-environment" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>OS version: CentOS 8.4, Ubuntu 20.04</p></li>
<li><p>Python version: 3.7, 3.8, 3.9, 3.10</p></li>
</ul>
<table class="docutils">
<thead>
  <tr>
    <th>Framework</th>
    <th>TensorFlow</th>
    <th>Intel TensorFlow</th>
    <th>PyTorch</th>
    <th>IPEX</th>
    <th>ONNX Runtime</th>
    <th>MXNet</th>
  </tr>
</thead>
<tbody>
  <tr align="center">
    <th>Version</th>
    <td class="tg-7zrl"><a href=https://github.com/tensorflow/tensorflow/tree/v2.10.0>2.10.0</a><br>
    <a href=https://github.com/tensorflow/tensorflow/tree/v2.9.1>2.9.1</a><br>
    <a href=https://github.com/tensorflow/tensorflow/tree/v2.8.2>2.8.2</a><br>
    <td class="tg-7zrl"><a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.10.0>2.10.0</a><br>
    <a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.9.1>2.9.1</a><br>
    <a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.8.0>2.8.0</a><br>
    <td class="tg-7zrl"><a href=https://download.pytorch.org/whl/torch_stable.html>1.12.1+cpu</a><br>
    <a href=https://download.pytorch.org/whl/torch_stable.html>1.11.0+cpu</a><br>
    <a href=https://download.pytorch.org/whl/torch_stable.html>1.10.0+cpu</a></td>
    <td class="tg-7zrl"><a href=https://github.com/intel/intel-extension-for-pytorch/tree/v1.12.0>1.12.0</a><br>
    <a href=https://github.com/intel/intel-extension-for-pytorch/tree/1.11.0>1.11.0</a><br>
    <a href=https://github.com/intel/intel-extension-for-pytorch/tree/v1.10.0>1.10.0</a></td>
    <td class="tg-7zrl"><a href=https://github.com/microsoft/onnxruntime/tree/v1.12.1>1.12.1</a><br>
    <a href=https://github.com/microsoft/onnxruntime/tree/v1.11.0>1.11.0</a><br>
    <a href=https://github.com/microsoft/onnxruntime/tree/v1.10.0>1.10.0</a></td>
    <td class="tg-7zrl"><a href=https://github.com/apache/incubator-mxnet/tree/1.8.0>1.8.0</a><br>
    <a href=https://github.com/apache/incubator-mxnet/tree/1.7.0>1.7.0</a><br>
    <a href=https://github.com/apache/incubator-mxnet/tree/1.6.0>1.6.0</a></td>
  </tr>
</tbody>
</table><blockquote>
<div><p><strong>Note:</strong>
Set the environment variable <code class="docutils literal notranslate"><span class="pre">TF_ENABLE_ONEDNN_OPTS=1</span></code> to enable oneDNN optimizations if you are using TensorFlow v2.6 to v2.8. oneDNN is the default for TensorFlow v2.9.</p>
</div></blockquote>
</div>
<div class="section" id="validated-models">
<h3>Validated Models<a class="headerlink" href="#validated-models" title="Permalink to this headline">¶</a></h3>
<p>Intel® Neural Compressor validated 420+ <a class="reference external" href="https://github.com/intel/neural-compressor/tree/566a9c1f0709a21aa8bed27c39ca2f25bd6ad783/./examples">examples</a> for quantization with a performance speedup geomean of 2.2x and up to 4.2x on VNNI while minimizing accuracy loss. Over 30 pruning and knowledge distillation samples are also available. More details for validated models are available <a class="reference internal" href="docs/validated_model_list.html"><span class="doc">here</span></a>.</p>
<div style = "width: 77%; margin-bottom: 2%;">
  <a target="_blank" href="./docs/imgs/release_data.png">
    <img src="./docs/imgs/release_data.png" alt="Architecture" width=800 height=500>
  </a>
</div></div>
</div>
<div class="section" id="documentation">
<h2>Documentation<a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h2>
<table class="docutils">
  <thead>
  <tr>
    <th colspan="9">Overview</th>
  </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="3" align="center"><a href="docs/design.html">Architecture</a></td>
      <td colspan="2" align="center"><a href="https://github.com/intel/neural-compressor/tree/master/examples">Examples</a></td>
      <td colspan="2" align="center"><a href="docs/bench.html">GUI</a></td>
      <td colspan="2" align="center"><a href="docs/api-introduction.html">APIs</a></td>
    </tr>
    <tr>
      <td colspan="5" align="center"><a href="https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html">Intel oneAPI AI Analytics Toolkit</a></td>
      <td colspan="4" align="center"><a href="https://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics">AI and Analytics Samples</a></td>
    </tr>
  </tbody>
  <thead>
  <tr>
    <th colspan="9">Basic API</th>
  </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="2" align="center"><a href="docs/transform.html">Transform</a></td>
      <td colspan="2" align="center"><a href="docs/dataset.html">Dataset</a></td>
      <td colspan="2" align="center"><a href="docs/metric.html">Metric</a></td>
      <td colspan="3" align="center"><a href="docs/objective.html">Objective</a></td>
    </tr>
  </tbody>
  <thead>
    <tr>
      <th colspan="9">Deep Dive</th>
    </tr>
  </thead>
  <tbody>
    <tr>
        <td colspan="2" align="center"><a href="docs/Quantization.html">Quantization</a></td>
        <td colspan="1" align="center"><a href="docs/pruning.html">Pruning</a> <a href="docs/sparsity.html">(Sparsity)</a> </td> 
        <td colspan="2" align="center"><a href="docs/distillation.html">Knowledge Distillation</a></td>
        <td colspan="2" align="center"><a href="docs/mixed_precision.html">Mixed Precision</a></td>
        <td colspan="2" align="center"><a href="docs/orchestration.html">Orchestration</a></td>
    </tr>
    <tr>
        <td colspan="2" align="center"><a href="docs/benchmark.html">Benchmarking</a></td>
        <td colspan="3" align="center"><a href="docs/distributed.html">Distributed Training</a></td>
        <td colspan="2" align="center"><a href="docs/model_conversion.html">Model Conversion</a></td>
        <td colspan="2" align="center"><a href="docs/tensorboard.html">TensorBoard</a></td>
    </tr>
    <tr>
        <td colspan="4" align="center"><a href="docs/distillation_quantization.html">Distillation for Quantization</a></td>
        <td colspan="5" align="center"><a href="neural_coder">Neural Coder</a></td>
    </tr>      </tbody>
  <thead>
      <tr>
        <th colspan="9">Advanced Topics</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td colspan="3" align="center"><a href="docs/adaptor.html">Adaptor</a></td>
          <td colspan="3" align="center"><a href="docs/tuning_strategies.html">Strategy</a></td>
          <td colspan="3" align="center"><a href="docs/reference_examples.html">Reference Example</a></td>
      </tr>
  </tbody>
</table></div>
<div class="section" id="selected-publications">
<h2>Selected Publications<a class="headerlink" href="#selected-publications" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/innovation-of-ai-software-extension-tensorflow.html">Meet the Innovation of Intel AI Software: Intel® Extension for TensorFlow*</a> (Oct 2022)</p></li>
<li><p><a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-inference-with-intel-neural-compressor.html#gs.gnq0cj">PyTorch* Inference Acceleration with Intel® Neural Compressor</a> (Oct 2022)</p></li>
<li><p>Neural Coder, a new plug-in for Intel Neural Compressor was covered by <a class="reference external" href="https://intel.sharepoint.com/sites/iLit-MyLearning/Shared%20Documents/My%20Learning/Haihao/MSR/2022/o%09https:/twitter.com/IntelDevTools/status/1583629213697212416?s=20&amp;t=f2dXY_g9-bCO-GQm-S6Btg">Twitter</a>, <a class="reference external" href="https://www.linkedin.com/posts/intel-software_oneapi-ai-deeplearning-activity-6989377309917007872-Dbzg?utm_source=share&amp;utm_medium=member_desktop">LinkedIn</a>, and <a class="reference external" href="https://mp.weixin.qq.com/s/LL-4eD-R0YagFgODM23oQA">Intel Developer Zone</a> from Intel, and <a class="reference external" href="https://twitter.com/IntelDevTools/status/1583629213697212416/retweets">Twitter</a> and <a class="reference external" href="https://www.linkedin.com/feed/update/urn:li:share:6990377841435574272/">LinkedIn</a> from Hugging Face. (Oct 2022)</p></li>
<li><p>Intel Neural Compressor successfully landed on <a class="reference external" href="https://console.cloud.google.com/marketplace/product/bitnami-launchpad/inc-tensorflow-intel?project=verdant-sensor-286207">GCP</a>, <a class="reference external" href="https://aws.amazon.com/marketplace/pp/prodview-yjyh2xmggbmga#pdp-support">AWS</a>, and <a class="reference external" href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bitnami.inc-tensorflow-intel">Azure</a> marketplace. (Oct 2022)</p></li>
<li><p><a class="reference external" href="https://twitter.com/i/status/1574909338203967497">Neural Coder (Intel Neural Compressor Plug-in): One-Click, No-Code Solution (Pat’s Keynote IntelON 2022)</a> (Sep 2022)</p></li>
<li><p><a class="reference external" href="https://medium.com/intel-analytics-software/alibaba-cloud-collaborates-with-intel-neural-compressor-for-better-productivity-and-performance-83cdb6500420">Alibaba Cloud and Intel Neural Compressor Deliver Better Productivity for PyTorch Users</a> (Sep 2022)</p></li>
<li><p><a class="reference external" href="https://medium.com/intel-analytics-software/efficient-text-classification-with-intel-neural-compressor-4853296deeac">Efficient Text Classification with Intel Neural Compressor</a> (Sep 2022)</p></li>
</ul>
<blockquote>
<div><p>View our <a class="reference internal" href="docs/publication_list.html"><span class="doc">full publication list</span></a>.</p>
</div></blockquote>
</div>
<div class="section" id="additional-content">
<h2>Additional Content<a class="headerlink" href="#additional-content" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="docs/releases_info.html"><span class="doc">Release Information</span></a></p></li>
<li><p><a class="reference internal" href="docs/contributions.html"><span class="doc">Contribution Guidelines</span></a></p></li>
<li><p><a class="reference internal" href="docs/legal_information.html"><span class="doc">Legal Information</span></a></p></li>
<li><p><a class="reference internal" href="SECURITY.html"><span class="doc">Security Policy</span></a></p></li>
<li><p><a class="reference external" href="https://intel.github.io/neural-compressor">Intel® Neural Compressor Website</a></p></li>
</ul>
</div>
<div class="section" id="hiring">
<h2>Hiring<a class="headerlink" href="#hiring" title="Permalink to this headline">¶</a></h2>
<p>We are actively hiring. Send your resume to inc.maintainers&#64;intel.com if you are interested in model compression techniques.</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Intel® Neural Compressor Documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="docs/examples_readme.html" class="btn btn-neutral float-right" title="Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>