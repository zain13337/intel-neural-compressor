:py:mod:`neural_compressor.strategy.mse`
========================================

.. py:module:: neural_compressor.strategy.mse

.. autoapi-nested-parse::

   MSE tuning strategy.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.strategy.mse.MSETuneStrategy




.. py:class:: MSETuneStrategy(model, conf, q_dataloader, q_func=None, eval_dataloader=None, eval_func=None, dicts=None, q_hooks=None)

   Bases: :py:obj:`neural_compressor.strategy.strategy.TuneStrategy`

   The tuning strategy using MSE policy in tuning space.

   The MSE strategy needs to get the tensors for each OP of raw FP32 models and the quantized model based on
   the best model-wise tuning configuration. It then calculates the MSE (Mean Squared Error) for each OP, sorts
   those OPs according to the MSE value, and performs the op-wise fallback in this order.

   .. py:method:: mse_impact_lst(op_list: List, fp32_model, best_qmodel)

      Calculate and generate the MSE impact list.

      :param op_list: List of ops in format of [(op_name, op_type), ...].
      :type op_list: List[Tuple(str, str)]
      :param fp32_model: The original FP32 model before quantization.
      :type fp32_model: Model
      :param current_best_model: The currently best quantized model.
      :type current_best_model: Model

      :returns:

                The sorted list of ops by its MSE
                  impaction, in the same format of 'op_list'.
      :rtype: ordered_op_name_types (List[Tuple(str, str)])


   .. py:method:: next_tune_cfg()

      Generate and yield the next tuning config.

      :Yields: *tune_config (dict)* -- A dict containing the tuning configuration for quantization.



