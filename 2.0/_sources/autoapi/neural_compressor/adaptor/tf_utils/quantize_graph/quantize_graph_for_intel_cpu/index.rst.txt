:py:mod:`neural_compressor.adaptor.tf_utils.quantize_graph.quantize_graph_for_intel_cpu`
========================================================================================

.. py:module:: neural_compressor.adaptor.tf_utils.quantize_graph.quantize_graph_for_intel_cpu

.. autoapi-nested-parse::

   Convert fp32 op to int8 and fuse the pattern.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.adaptor.tf_utils.quantize_graph.quantize_graph_for_intel_cpu.QuantizeGraphForIntel




.. py:class:: QuantizeGraphForIntel(input_graph, input_node_names, output_node_names, op_wise_config, op_wise_sequences, device, fake_quant=False, new_api=False, performance_only=False, itex_mode=False)

   Bases: :py:obj:`neural_compressor.adaptor.tf_utils.quantize_graph.quantize_graph_base.QuantizeGraphBase`

   Quantize the graph.

   .. py:method:: do_transform()

      Apply all the transformers to fuse into int8 op.



