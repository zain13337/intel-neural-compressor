:py:mod:`neural_compressor.adaptor.tf_utils.quantize_graph.qdq.fuse_qdq_bn`
===========================================================================

.. py:module:: neural_compressor.adaptor.tf_utils.quantize_graph.qdq.fuse_qdq_bn

.. autoapi-nested-parse::

   Quantize FusedBatchNormV3 to int8 op.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.adaptor.tf_utils.quantize_graph.qdq.fuse_qdq_bn.FuseNodeStartWithFusedBatchNormV3




.. py:class:: FuseNodeStartWithFusedBatchNormV3(**kwargs)

   Bases: :py:obj:`neural_compressor.adaptor.tf_utils.quantize_graph.quantize_graph_base.QuantizeNodeBase`

   Quantize FusedBatchNormV3 to int8 op _QuantizedFusedBatchNorm.

   .. py:method:: apply_newly_bn_relu_fusion(match_node_name)

      Apply the BN + Relu fusion.


   .. py:method:: apply_newly_bn_leakyrelu_fusion(match_node_name)

      Apply BN + LeakyRelu fusion.


   .. py:method:: get_longest_fuse()

      Get the longest fusion pattern.


   .. py:method:: apply_the_transform()

      Apply the BN int8 fusion.



