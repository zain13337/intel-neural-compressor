

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Intel® Neural Compressor &mdash; Intel® Neural Compressor  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Examples" href="examples_readme.html" />
    <link rel="prev" title="Intel® Neural Compressor Documentation" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Intel® Neural Compressor
          

          
          </a>

          
            
            
            <div class="version">
              <a href="../versions.html">2.0▼</a>
              <p>Click link above to switch version</p>
            </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Intel® Neural Compressor</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-on-linux">Install on Linux</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quantization-with-python-api">Quantization with Python API</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantization-with-jupyterlab-extension">Quantization with JupyterLab Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantization-with-gui">Quantization with GUI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#system-requirements">System Requirements</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#validated-hardware-environment">Validated Hardware Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#intel-neural-compressor-supports-cpus-based-on-intel-64-architecture-or-compatible-processors">Intel® Neural Compressor supports CPUs based on Intel 64 architecture or compatible processors:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel-neural-compressor-supports-gpus-built-on-intel-s-xe-architecture">Intel® Neural Compressor supports GPUs built on Intel’s Xe architecture:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel-neural-compressor-quantized-onnx-models-support-multiple-hardware-vendors-through-onnx-runtime">Intel® Neural Compressor quantized ONNX models support multiple hardware vendors through ONNX Runtime:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#validated-software-environment">Validated Software Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#validated-models">Validated Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#selected-publications-events">Selected Publications/Events</a></li>
<li class="toctree-l2"><a class="reference internal" href="#additional-content">Additional Content</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hiring">Hiring</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-documentation/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Intel® Neural Compressor repository</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Intel® Neural Compressor</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Intel® Neural Compressor</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/Welcome.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div align="center"><section id="intel-neural-compressor">
<h1>Intel® Neural Compressor<a class="headerlink" href="#intel-neural-compressor" title="Permalink to this heading">¶</a></h1>
<p><h3> An open-source Python library supporting popular model compression techniques on all mainstream deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet)</h3></p>
<p><a class="reference external" href="https://github.com/intel/neural-compressor"><img alt="python" src="https://img.shields.io/badge/python-3.7%2B-blue" /></a>
<a class="reference external" href="https://github.com/intel/neural-compressor/releases"><img alt="version" src="https://img.shields.io/badge/release-2.0-green" /></a>
<a class="reference external" href="https://github.com/intel/neural-compressor/blob/master/LICENSE"><img alt="license" src="https://img.shields.io/badge/license-Apache%202-blue" /></a>
<a class="reference external" href="https://github.com/intel/neural-compressor"><img alt="coverage" src="https://img.shields.io/badge/coverage-90%25-green" /></a>
<a class="reference external" href="https://pepy.tech/project/neural-compressor"><img alt="Downloads" src="https://static.pepy.tech/personalized-badge/neural-compressor?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=green&amp;left_text=downloads" /></a></p>
</div><hr class="docutils" />
<div align="left"><p>Intel® Neural Compressor, formerly known as Intel® Low Precision Optimization Tool, is an open-source Python library that runs on Intel CPUs and GPUs, which delivers unified interfaces across multiple deep-learning frameworks for popular network compression technologies such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help the user quickly find out the best quantized model. It also implements different weight-pruning algorithms to generate a pruned model with predefined sparsity goal. It also supports knowledge distillation to distill the knowledge from the teacher model to the student model.
Intel® Neural Compressor is a critical AI software component in the <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html">Intel® oneAPI AI Analytics Toolkit</a>.</p>
<p><strong>Visit the Intel® Neural Compressor online document website at: <a class="reference external" href="https://intel.github.io/neural-compressor">https://intel.github.io/neural-compressor</a>.</strong></p>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">¶</a></h2>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">¶</a></h3>
<p>Python version: 3.7, 3.8, 3.9, 3.10</p>
</section>
<section id="install-on-linux">
<h3>Install on Linux<a class="headerlink" href="#install-on-linux" title="Permalink to this heading">¶</a></h3>
<ul>
<li><p>Release binary install</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># install stable basic version from pypi</span>
pip<span class="w"> </span>install<span class="w"> </span>neural-compressor
<span class="c1"># or install stable full version from pypi (including GUI)</span>
pip<span class="w"> </span>install<span class="w"> </span>neural-compressor-full
</pre></div>
</div>
</li>
<li><p>Nightly binary install</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/intel/neural-compressor.git
<span class="nb">cd</span><span class="w"> </span>neural-compressor
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
<span class="c1"># install nightly basic version from pypi</span>
pip<span class="w"> </span>install<span class="w"> </span>-i<span class="w"> </span>https://test.pypi.org/simple/<span class="w"> </span>neural-compressor
<span class="c1"># or install nightly full version from pypi (including GUI)</span>
pip<span class="w"> </span>install<span class="w"> </span>-i<span class="w"> </span>https://test.pypi.org/simple/<span class="w"> </span>neural-compressor-full
</pre></div>
</div>
</li>
</ul>
<p>More installation methods can be found at <a class="reference external" href="./installation_guide.html">Installation Guide</a>. Please check out our <a class="reference external" href="./faq.html">FAQ</a> for more details.</p>
</section>
</section>
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this heading">¶</a></h2>
<section id="quantization-with-python-api">
<h3>Quantization with Python API<a class="headerlink" href="#quantization-with-python-api" title="Permalink to this heading">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># A TensorFlow Example</span>
pip<span class="w"> </span>install<span class="w"> </span>tensorflow
<span class="c1"># Prepare fp32 model</span>
wget<span class="w"> </span>https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6/mobilenet_v1_1.0_224_frozen.pb
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.config</span> <span class="kn">import</span> <span class="n">PostTrainingQuantConfig</span>
<span class="kn">from</span> <span class="nn">neural_compressor.data.dataloaders.dataloader</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">neural_compressor.data</span> <span class="kn">import</span> <span class="n">Datasets</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">Datasets</span><span class="p">(</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">)[</span><span class="s1">&#39;dummy&#39;</span><span class="p">](</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="kn">from</span> <span class="nn">neural_compressor.quantization</span> <span class="kn">import</span> <span class="n">fit</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">PostTrainingQuantConfig</span><span class="p">()</span>
<span class="n">fit</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="s2">&quot;./mobilenet_v1_1.0_224_frozen.pb&quot;</span><span class="p">,</span>
  <span class="n">conf</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
  <span class="n">calib_dataloader</span><span class="o">=</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">framework</span><span class="o">=</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">),</span>
  <span class="n">eval_dataloader</span><span class="o">=</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">framework</span><span class="o">=</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="quantization-with-jupyterlab-extension">
<h3>Quantization with <a class="reference external" href="./neural_coder/extensions/neural_compressor_ext_lab/README.html">JupyterLab Extension</a><a class="headerlink" href="#quantization-with-jupyterlab-extension" title="Permalink to this heading">¶</a></h3>
<p>Search for <code class="docutils literal notranslate"><span class="pre">jupyter-lab-neural-compressor</span></code> in the Extension Manager in JupyterLab and install with one click:</p>
<a target="_blank" href="imgs/extmanager.png">
  <img src="imgs/extmanager.png" alt="Extension" width="35%" height="35%">
</a></section>
<section id="quantization-with-gui">
<h3>Quantization with <a class="reference external" href="./bench.html">GUI</a><a class="headerlink" href="#quantization-with-gui" title="Permalink to this heading">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># An ONNX Example</span>
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">onnx</span><span class="o">==</span><span class="m">1</span>.12.0<span class="w"> </span><span class="nv">onnxruntime</span><span class="o">==</span><span class="m">1</span>.12.1<span class="w"> </span>onnxruntime-extensions
<span class="c1"># Prepare fp32 model</span>
wget<span class="w"> </span>https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet50-v1-12.onnx
<span class="c1"># Start GUI</span>
inc_bench
</pre></div>
</div>
<a target="_blank" href="./_static/imgs/INC_GUI.gif">
  <img src="./_static/imgs/INC_GUI.gif" alt="Architecture">
</a></section>
</section>
<section id="system-requirements">
<h2>System Requirements<a class="headerlink" href="#system-requirements" title="Permalink to this heading">¶</a></h2>
<section id="validated-hardware-environment">
<h3>Validated Hardware Environment<a class="headerlink" href="#validated-hardware-environment" title="Permalink to this heading">¶</a></h3>
<section id="intel-neural-compressor-supports-cpus-based-on-intel-64-architecture-or-compatible-processors">
<h4>Intel® Neural Compressor supports CPUs based on <a class="reference external" href="https://en.wikipedia.org/wiki/X86-64">Intel 64 architecture or compatible processors</a>:<a class="headerlink" href="#intel-neural-compressor-supports-cpus-based-on-intel-64-architecture-or-compatible-processors" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Intel Xeon Scalable processor (formerly Skylake, Cascade Lake, Cooper Lake, Ice Lake, and Sapphire Rapids)</p></li>
<li><p>Intel Xeon CPU Max Series (formerly Sapphire Rapids HBM)</p></li>
</ul>
</section>
<section id="intel-neural-compressor-supports-gpus-built-on-intel-s-xe-architecture">
<h4>Intel® Neural Compressor supports GPUs built on Intel’s Xe architecture:<a class="headerlink" href="#intel-neural-compressor-supports-gpus-built-on-intel-s-xe-architecture" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Intel Data Center GPU Flex Series (formerly Arctic Sound-M)</p></li>
<li><p>Intel Data Center GPU Max Series (formerly Ponte Vecchio)</p></li>
</ul>
</section>
<section id="intel-neural-compressor-quantized-onnx-models-support-multiple-hardware-vendors-through-onnx-runtime">
<h4>Intel® Neural Compressor quantized ONNX models support multiple hardware vendors through ONNX Runtime:<a class="headerlink" href="#intel-neural-compressor-quantized-onnx-models-support-multiple-hardware-vendors-through-onnx-runtime" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Intel CPU, AMD/ARM CPU, and NVidia GPU. Please refer to the validated model <a class="reference external" href="./validated_model_list.html#Validated-ONNX-QDQ-INT8-models-on-multiple-hardware-through-ONNX-Runtime">list</a>.</p></li>
</ul>
</section>
</section>
<section id="validated-software-environment">
<h3>Validated Software Environment<a class="headerlink" href="#validated-software-environment" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>OS version: CentOS 8.4, Ubuntu 20.04</p></li>
<li><p>Python version: 3.7, 3.8, 3.9, 3.10</p></li>
</ul>
<table class="docutils">
<thead>
  <tr style="vertical-align: middle; text-align: center;">
    <th>Framework</th>
    <th>TensorFlow</th>
    <th>Intel<br>TensorFlow</th>
    <th>Intel®<br>Extension for<br>TensorFlow*</th>
    <th>PyTorch</th>
    <th>Intel®<br>Extension for<br>PyTorch*</th>
    <th>ONNX<br>Runtime</th>
    <th>MXNet</th>
  </tr>
</thead>
<tbody>
  <tr align="center">
    <th>Version</th>
    <td class="tg-7zrl"><a href=https://github.com/tensorflow/tensorflow/tree/v2.11.0>2.11.0</a><br>
    <a href=https://github.com/tensorflow/tensorflow/tree/v2.10.1>2.10.1</a><br>
    <a href=https://github.com/tensorflow/tensorflow/tree/v2.9.3>2.9.3</a><br></td>
    <td class="tg-7zrl"><a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.11.0>2.11.0</a><br>
    <a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.10.0>2.10.0</a><br>
    <a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.9.1>2.9.1</a><br></td>
    <td class="tg-7zrl"><a href=https://github.com/intel/intel-extension-for-tensorflow/tree/v1.0.0>1.0.0</a></td>
    <td class="tg-7zrl"><a href=https://download.pytorch.org/whl/torch_stable.html>1.13.1+cpu</a><br>
    <a href=https://download.pytorch.org/whl/torch_stable.html>1.12.1+cpu</a><br>
    <a href=https://download.pytorch.org/whl/torch_stable.html>1.11.0+cpu</a><br></td>
    <td class="tg-7zrl"><a href=https://github.com/intel/intel-extension-for-pytorch/tree/v1.13.0+cpu>1.13.0</a><br>
    <a href=https://github.com/intel/intel-extension-for-pytorch/tree/v1.12.100>1.12.1</a><br>
    <a href=https://github.com/intel/intel-extension-for-pytorch/tree/v1.11.0>1.11.0</a><br></td>
    <td class="tg-7zrl"><a href=https://github.com/microsoft/onnxruntime/tree/v1.13.1>1.13.1</a><br>
    <a href=https://github.com/microsoft/onnxruntime/tree/v1.12.1>1.12.1</a><br>
    <a href=https://github.com/microsoft/onnxruntime/tree/v1.11.0>1.11.0</a><br></td>
    <td class="tg-7zrl"><a href=https://github.com/apache/incubator-mxnet/tree/1.9.1>1.9.1</a><br>
    <a href=https://github.com/apache/incubator-mxnet/tree/1.8.0>1.8.0</a><br>
    <a href=https://github.com/apache/incubator-mxnet/tree/1.7.0>1.7.0</a><br></td>
  </tr>
</tbody>
</table><blockquote>
<div><p><strong>Note:</strong>
Set the environment variable <code class="docutils literal notranslate"><span class="pre">TF_ENABLE_ONEDNN_OPTS=1</span></code> to enable oneDNN optimizations if you are using TensorFlow v2.6 to v2.8. oneDNN is the default for TensorFlow v2.9.</p>
</div></blockquote>
</section>
<section id="validated-models">
<h3>Validated Models<a class="headerlink" href="#validated-models" title="Permalink to this heading">¶</a></h3>
<p>Intel® Neural Compressor validated the quantization for 10K+ models from popular model hubs (e.g., HuggingFace Transformers, Torchvision, TensorFlow Model Hub, ONNX Model Zoo) with the performance speedup up to 4.2x on VNNI while minimizing the accuracy loss. Over 30 pruning and knowledge distillation samples are also available. More details for validated typical models are available <a class="reference external" href="./validated_model_list.html">here</a>.</p>
</section>
</section>
<section id="documentation">
<h2>Documentation<a class="headerlink" href="#documentation" title="Permalink to this heading">¶</a></h2>
<table class="docutils">
  <thead>
  <tr>
    <th colspan="9">Overview</th>
  </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="4" align="center"><a href="./design.html#architecture">Architecture</a></td>
      <td colspan="3" align="center"><a href="./design.html#workflow">Workflow</a></td>
      <td colspan="1" align="center"><a href="https://intel.github.io/neural-compressor/api-documentation/apis.html">APIs</a></td>
      <td colspan="1" align="center"><a href="./bench.html">GUI</a></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><a href="./examples#notebook-examples">Notebook</a></td>
      <td colspan="1" align="center"><a href="./examples">Examples</a></td>
      <td colspan="1" align="center"><a href="./validated_model_list.html">Results</a></td>
      <td colspan="5" align="center"><a href="https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html">Intel oneAPI AI Analytics Toolkit</a></td>
    </tr>
  </tbody>
  <thead>
    <tr>
      <th colspan="9">Python-based APIs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
        <td colspan="2" align="center"><a href="./quantization.html">Quantization</a></td>
        <td colspan="3" align="center"><a href="./mixed_precision.html">Advanced Mixed Precision</a></td>
        <td colspan="2" align="center"><a href="./pruning.html">Pruning(Sparsity)</a></td> 
        <td colspan="2" align="center"><a href="./distillation.html">Distillation</a></td>
    </tr>
    <tr>
        <td colspan="2" align="center"><a href="./orchestration.html">Orchestration</a></td>        
        <td colspan="2" align="center"><a href="./benchmark.html">Benchmarking</a></td>
        <td colspan="3" align="center"><a href="./distributed.html">Distributed Compression</a></td>
        <td colspan="3" align="center"><a href="./export.html">Model Export</a></td>
    </tr>
  </tbody>
  <thead>
    <tr>
      <th colspan="9">Neural Coder (Zero-code Optimization)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
        <td colspan="1" align="center"><a href="./neural_coder/docs/PythonLauncher.html">Launcher</a></td>
        <td colspan="2" align="center"><a href="./neural_coder/extensions/neural_compressor_ext_lab/README.html">JupyterLab Extension</a></td>
        <td colspan="3" align="center"><a href="./neural_coder/extensions/neural_compressor_ext_vscode/README.html">Visual Studio Code Extension</a></td>
        <td colspan="3" align="center"><a href="./neural_coder/docs/SupportMatrix.html">Supported Matrix</a></td>
    </tr>    
  </tbody>
  <thead>
      <tr>
        <th colspan="9">Advanced Topics</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td colspan="1" align="center"><a href="./adaptor.html">Adaptor</a></td>
          <td colspan="2" align="center"><a href="./tuning_strategies.html">Strategy</a></td>
          <td colspan="3" align="center"><a href="./distillation_quantization.html">Distillation for Quantization</a></td>
          <td colspan="3" align="center">SmoothQuant (Coming Soon)</td>
      </tr>
  </tbody>
</table></section>
<section id="selected-publications-events">
<h2>Selected Publications/Events<a class="headerlink" href="#selected-publications-events" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>NeurIPS’2022: <a class="reference external" href="https://arxiv.org/abs/2211.07715">Fast Distilbert on CPUs</a> (Dec 2022)</p></li>
<li><p>NeurIPS’2022: <a class="reference external" href="https://arxiv.org/abs/2210.17114">QuaLA-MiniLM: a Quantized Length Adaptive MiniLM</a> (Dec 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/&#64;kawapanion/mlefficiency-optimizing-transformer-models-for-efficiency-a9e230cff051">MLefficiency — Optimizing transformer models for efficiency</a> (Dec 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/one-click-acceleration-of-huggingface-transformers-with-optimum-intel-by-neural-coder-f35ca3b1a82f">One-Click Acceleration of Hugging Face Transformers with Intel’s Neural Coder</a> (Dec 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/one-click-quantize-your-deep-learning-code-in-visual-studio-code-with-neural-coder-extension-8be1a0022c29">One-Click Quantization of Deep Learning Models with the Neural Coder Extension</a> (Dec 2022)</p></li>
<li><p>Blog on Medium: <a class="reference external" href="https://medium.com/intel-analytics-software/accelerating-stable-diffusion-inference-through-8-bit-post-training-quantization-with-intel-neural-e28f3615f77c">Accelerate Stable Diffusion with Intel Neural Compressor</a> (Dec 2022)</p></li>
</ul>
<blockquote>
<div><p>View our <a class="reference external" href="./publication_list.html">full publication list</a>.</p>
</div></blockquote>
</section>
<section id="additional-content">
<h2>Additional Content<a class="headerlink" href="#additional-content" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="./releases_info.html">Release Information</a></p></li>
<li><p><a class="reference external" href="./CONTRIBUTING.html">Contribution Guidelines</a></p></li>
<li><p><a class="reference external" href="./legal_information.html">Legal Information</a></p></li>
<li><p><a class="reference external" href="SECURITY.html">Security Policy</a></p></li>
<li><p><a class="reference external" href="https://intel.github.io/neural-compressor">Intel® Neural Compressor Website</a></p></li>
</ul>
</section>
<section id="hiring">
<h2>Hiring<a class="headerlink" href="#hiring" title="Permalink to this heading">¶</a></h2>
<p>We are actively hiring. Send your resume to inc.maintainers&#64;intel.com if you are interested in model compression techniques.</p>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="examples_readme.html" class="btn btn-neutral float-right" title="Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index.html" class="btn btn-neutral float-left" title="Intel® Neural Compressor Documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, Intel® Neural Compressor.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>