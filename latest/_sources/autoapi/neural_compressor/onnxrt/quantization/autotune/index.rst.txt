:orphan:

:py:mod:`neural_compressor.onnxrt.quantization.autotune`
========================================================

.. py:module:: neural_compressor.onnxrt.quantization.autotune


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   neural_compressor.onnxrt.quantization.autotune.get_all_config_set
   neural_compressor.onnxrt.quantization.autotune.autotune




.. py:function:: autotune(model_input: Union[pathlib.Path, str], tune_config: neural_compressor.common.base_tuning.TuningConfig, eval_fn: Callable, eval_args: Optional[Tuple[Any]] = None, calibration_data_reader: neural_compressor.onnxrt.quantization.calibrate.CalibrationDataReader = None) -> Union[None, onnx.ModelProto]

   The main entry of auto-tune.

   :param model_input: onnx model path.
   :type model_input: Union[Path, str]
   :param tune_config: tuning config.
                       TuningConfig is created with algorithm configs, parameters supported tuning are in their params_list.
                       Support:
                       Expand parameters to a list of parameters like TuningConfig(config_set=[RTNConfig(weight_bits=[4, 8])])
                       Pass a list of configs like TuningConfig(config_set=[RTNConfig(), GPTQConfig()])
   :type tune_config: TuningConfig
   :param eval_fn: evaluate function.
                   During evaluation, autotune will only pass model path as the input of function.
   :type eval_fn: Callable
   :param eval_args: evaluate arguments.
                     Positional arguments for `eval_fn`.
   :type eval_args: Optional[Tuple[Any]]
   :param calibration_data_reader: dataloader for calibration.
   :type calibration_data_reader: CalibrationDataReader


