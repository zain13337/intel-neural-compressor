<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>neural_compressor.conf.pythonic_config &mdash; Intel® Neural Compressor 2.1 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../../../versions.html">2.1▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/source/get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/source/installation_guide.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/source/user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/source/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/source/api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/source/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/source/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/source/SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><code class="xref py py-mod docutils literal notranslate"><span class="pre">neural_compressor.conf.pythonic_config</span></code></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/autoapi/neural_compressor/conf/pythonic_config/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-neural_compressor.conf.pythonic_config">
<span id="neural-compressor-conf-pythonic-config"></span><h1><a class="reference internal" href="#module-neural_compressor.conf.pythonic_config" title="neural_compressor.conf.pythonic_config"><code class="xref py py-mod docutils literal notranslate"><span class="pre">neural_compressor.conf.pythonic_config</span></code></a><a class="headerlink" href="#module-neural_compressor.conf.pythonic_config" title="Permalink to this heading"></a></h1>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this heading"></a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#neural_compressor.conf.pythonic_config.QuantizationConfig" title="neural_compressor.conf.pythonic_config.QuantizationConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizationConfig</span></code></a></p></td>
<td><p>Basic class for quantization config. Inherited by PostTrainingQuantConfig and QuantizationAwareTrainingConfig.</p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt class="sig sig-object py" id="neural_compressor.conf.pythonic_config.QuantizationConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neural_compressor.conf.pythonic_config.</span></span><span class="sig-name descname"><span class="pre">QuantizationConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approach</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'post_training_static_quant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calibration_sampling_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[100]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op_type_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op_name_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'basic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">objective</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'performance'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">performance_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bf16</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accuracy_criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">accuracy_criterion</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_distributed_tuning</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/intel/neural-compressor/blob/v2.1/neural_compressor/conf/pythonic_config.py"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neural_compressor.conf.pythonic_config.QuantizationConfig" title="Permalink to this definition"></a></dt>
<dd><p>Basic class for quantization config. Inherited by PostTrainingQuantConfig and QuantizationAwareTrainingConfig.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – Inputs of model, only required in tensorflow.</p></li>
<li><p><strong>outputs</strong> – Outputs of model, only required in tensorflow.</p></li>
<li><p><strong>backend</strong> – Backend for model execution. Support ‘default’, ‘itex’, ‘ipex’, ‘onnxrt_trt_ep’, ‘onnxrt_cuda_ep’</p></li>
<li><p><strong>domain</strong> – Model domain. Support ‘auto’, ‘cv’, ‘object_detection’, ‘nlp’ and ‘recommendation_system’.
Adaptor will use specific quantization settings for different domains automatically, and
explicitly specified quantization settings will override the automatic setting.
If users set domain as auto, automatic detection for domain will be executed.</p></li>
<li><p><strong>recipes</strong> – <p>Recipes for quantiztaion, support list is as below.
‘smooth_quant’: whether do smooth quant
‘smooth_quant_args’: parameters for smooth_quant
‘fast_bias_correction’: whether do fast bias correction
‘weight_correction’: whether do weight correction
‘gemm_to_matmul’: whether convert gemm to matmul and add, only valid for onnx models
‘graph_optimization_level’: support ‘DISABLE_ALL’, ‘ENABLE_BASIC’, ‘ENABLE_EXTENDED’, ‘ENABLE_ALL’</p>
<blockquote>
<div><p>only valid for onnx models</p>
</div></blockquote>
<p>’first_conv_or_matmul_quantization’: whether quantize the first conv or matmul
‘last_conv_or_matmul_quantization’: whether quantize the last conv or matmul
‘pre_post_process_quantization’: whether quantize the ops in preprocess and postprocess
‘add_qdq_pair_to_weight’: whether add QDQ pair for weights, only vaild for onnxrt_trt_ep
‘optypes_to_exclude_output_quant’: don’t quantize output of specified optypes
‘dedicated_qdq_pair’: whether dedicate QDQ pair, only vaild for onnxrt_trt_ep</p>
</p></li>
<li><p><strong>quant_format</strong> – Support ‘default’, ‘QDQ’ and ‘QOperator’, only required in ONNXRuntime.</p></li>
<li><p><strong>device</strong> – Support ‘cpu’ and ‘gpu’.</p></li>
<li><p><strong>calibration_sampling_size</strong> – Number of calibration sample.</p></li>
<li><p><strong>op_type_dict</strong> – <p>Tuning constraints on optype-wise  for advance user to reduce tuning space.
User can specify the quantization config by op type:
example:
{</p>
<blockquote>
<div><dl>
<dt>’Conv’: {</dt><dd><dl class="simple">
<dt>‘weight’: {</dt><dd><p>‘dtype’: [‘fp32’]</p>
</dd>
</dl>
<p>},
‘activation’: {</p>
<blockquote>
<div><p>’dtype’: [‘fp32’]</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>
</p></li>
<li><p><strong>op_name_dict</strong> – <p>Tuning constraints on op-wise for advance user to reduce tuning space.
User can specify the quantization config by op name:
example:
{</p>
<blockquote>
<div><dl>
<dt>”layer1.0.conv1”: {</dt><dd><dl class="simple">
<dt>“activation”: {</dt><dd><p>“dtype”: [“fp32”]</p>
</dd>
</dl>
<p>},
“weight”: {</p>
<blockquote>
<div><p>”dtype”: [“fp32”]</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>},</p>
</div></blockquote>
<p>}</p>
</p></li>
<li><p><strong>strategy</strong> – Strategy name used in tuning, Please refer to docs/source/tuning_strategies.md.</p></li>
<li><p><strong>strategy_kwargs</strong> – Parameters for strategy, Please refer to docs/source/tuning_strategies.md.</p></li>
<li><p><strong>objective</strong> – Objective with accuracy constraint guaranteed, support ‘performance’, ‘modelsize’, ‘footprint’.
Please refer to docs/source/objective.md.
Default value is ‘performance’.</p></li>
<li><p><strong>timeout</strong> – Tuning timeout (seconds). default value is 0 which means early stop</p></li>
<li><p><strong>max_trials</strong> – Max tune times. default value is 100. Combine with timeout field to decide when to exit</p></li>
<li><p><strong>performance_only</strong> – Whether do evaluation</p></li>
<li><p><strong>reduce_range</strong> – Whether use 7 bit to quantization.</p></li>
<li><p><strong>example_inputs</strong> – Used to trace PyTorch model with torch.jit/torch.fx.</p></li>
<li><p><strong>excluded_precisions</strong> – Precisions to be excluded, Default value is empty list.
Neural compressor enable the mixed precision with fp32 + bf16 + int8 by default.
If you want to disable bf16 data type, you can specify excluded_precisions = [‘bf16].</p></li>
<li><p><strong>quant_level</strong> – Support auto, 0 and 1, 0 is conservative strategy, 1 is basic or user-specified
strategy, auto (default) is the combination of 0 and 1.</p></li>
<li><p><strong>accuracy_criterion</strong> – Accuracy constraint settings.</p></li>
<li><p><strong>use_distributed_tuning</strong> – Whether use distributed tuning or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>