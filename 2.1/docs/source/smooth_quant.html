<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Smooth Quant &mdash; Intel® Neural Compressor 2.1 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../versions.html">2.1▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation_guide.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Smooth Quant</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/source/smooth_quant.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="smooth-quant">
<h1>Smooth Quant<a class="headerlink" href="#smooth-quant" title="Permalink to this heading"></a></h1>
<ol class="simple">
<li><p><a class="reference external" href="#Introduction">Introduction</a></p></li>
<li><p><a class="reference external" href="#Quantization-Fundamentals">Quantization Fundamentals</a></p></li>
<li><p><a class="reference external" href="#SmoothQuant-and-Our-Enhancement">SmoothQuant and Our Enhancement</a></p></li>
<li><p><a class="reference external" href="#Validated-Models">Validated Models</a></p></li>
<li><p><a class="reference external" href="#Example">Example</a></p></li>
</ol>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h2>
<p>Quantization is a common compression operation to reduce memory and accelerate inference by converting the floating point matrix to an integer matrix. For large language models (LLMs) with gigantic parameters, the systematic outliers make quantification of activations difficult.  <a class="reference external" href="https://arxiv.org/abs/2211.10438">SmoothQuant</a>, a training free post-training quantization (PTQ) solution, offline migrates this difficulty from activations to weights with a mathematically equivalent transformation.</p>
</section>
<section id="quantization-fundamentals">
<h2>Quantization Fundamentals<a class="headerlink" href="#quantization-fundamentals" title="Permalink to this heading"></a></h2>
<p>Quantization is a common compression operation to reduce memory and accelerate inference; therefore, the difficulty of LLM deployment can be alleviated. Quantization converts the floating point matrix to an integer matrix.</p>
<p>The equation of quantization is as follows:</p>
<p>$$
X_{int8} = round(X_{fp32}/S) + Z \tag{1}
$$</p>
<p>where $X_{fp32}$ is the input matrix, $S$ is the scale factor,  $Z$ is the integer zero point.</p>
<section id="per-tenor-per-channel">
<h3>Per-tenor &amp; Per-channel<a class="headerlink" href="#per-tenor-per-channel" title="Permalink to this heading"></a></h3>
<p>There are several choices of sharing quantization parameters among tensor elements, also called quantization granularity. The coarsest level, per-tensor granularity, is that all elements in the tensor share the same quantization parameters. Finer granularity means sharing quantization parameters per row or per column for 2D matrices and per channel for 3D matrices. Similarly, the finest granularity is that each element has an individual parameter.</p>
<p>However, due to the model accuracy and computational consumption, per-tensor or per-channel are usually adopted. <strong>In the following part, We will show per-channel could bring lower quantization loss but with some limitations, that is why normally we use per-channel for weight quantization and per-tensor for activation/input quantization</strong></p>
<section id="per-tensor-example">
<h4>Per-tensor example<a class="headerlink" href="#per-tensor-example" title="Permalink to this heading"></a></h4>
<p>Suppose the weight tensor is：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.6839</span><span class="p">,</span> <span class="mf">0.4741</span><span class="p">,</span> <span class="mf">0.7451</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.9301</span><span class="p">,</span> <span class="mf">0.1742</span><span class="p">,</span> <span class="mf">0.6835</span><span class="p">]]</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>According to the formula (1), we need to scale $S$ and zero point $Z$ to calculate the integer matrix.</p>
<p>$$
S = \frac{X_{max} - X{min}}{2^b -1} \tag{2}
$$</p>
<p>$$
Z = -round(X_{min/}/S) \tag{3}
$$</p>
<p>The per-tensor quantization function is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    <span class="n">q_min</span><span class="p">,</span> <span class="n">q_max</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">2.</span> <span class="o">**</span> <span class="n">num_bits</span> <span class="o">-</span> <span class="mf">1.</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">0</span> <span class="o">-</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span>
    <span class="n">q_x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">bias</span>
    <span class="n">q_x</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">q_min</span><span class="p">,</span> <span class="n">q_max</span><span class="p">)</span><span class="o">.</span><span class="n">round_</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;scale = </span><span class="si">{</span><span class="n">scale</span><span class="si">}</span><span class="s1">, bias = </span><span class="si">{</span><span class="n">bias</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q_x</span>
</pre></div>
</div>
<p>Then we can get the quantized $W_{q}$:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span><span class="nv">W_q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>quantize<span class="o">(</span>W<span class="o">)</span>
<span class="nv">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span>.00296431384049356,<span class="w"> </span><span class="nv">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>-59.0
&gt;&gt;&gt;<span class="w"> </span>W_q
tensor<span class="o">([[</span><span class="m">172</span>.,<span class="w"> </span><span class="m">101</span>.,<span class="w"> </span><span class="m">192</span>.<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">255</span>.,<span class="w">   </span><span class="m">0</span>.,<span class="w"> </span><span class="m">172</span>.<span class="o">]])</span>
</pre></div>
</div>
<p>With the value of scale and bias, we can dequantize the tensor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dequantize</span><span class="p">(</span><span class="n">q_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="p">(</span><span class="n">q_x</span> <span class="o">-</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span><span class="nv">W_dq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>dequantize<span class="o">(</span>W_dq,<span class="w"> </span><span class="m">0</span>.001,<span class="w"> </span>-50<span class="o">)</span>
&gt;&gt;&gt;<span class="w"> </span>W_dq
tensor<span class="o">([[</span><span class="m">0</span>.1220,<span class="w"> </span><span class="m">0</span>.0500,<span class="w"> </span><span class="m">0</span>.1430<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">0</span>.2570,<span class="w"> </span><span class="m">0</span>.0500,<span class="w"> </span><span class="m">0</span>.1890<span class="o">]])</span>
&gt;&gt;&gt;<span class="w"> </span><span class="nv">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.nn.MSELoss<span class="o">()(</span>W_dq,<span class="w"> </span>W<span class="o">)</span>
&gt;&gt;&gt;<span class="w"> </span>loss.item<span class="o">()</span>
<span class="m">0</span>.1983354538679123

&gt;&gt;&gt;<span class="w"> </span><span class="nv">W_dq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>dequantize<span class="o">(</span>W_q,<span class="w"> </span><span class="m">0</span>.0020850980654358864,<span class="w"> </span>-70<span class="o">)</span>
&gt;&gt;&gt;<span class="w"> </span>W_dq
tensor<span class="o">([[</span><span class="m">0</span>.6848,<span class="w"> </span><span class="m">0</span>.4743,<span class="w"> </span><span class="m">0</span>.7440<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">0</span>.9308,<span class="w"> </span><span class="m">0</span>.1749,<span class="w"> </span><span class="m">0</span>.6848<span class="o">]])</span>
&gt;&gt;&gt;<span class="w"> </span><span class="nv">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.nn.MSELoss<span class="o">()(</span>W_dq,<span class="w"> </span>W<span class="o">)</span>
&gt;&gt;&gt;<span class="w"> </span>loss.item<span class="o">()</span>
</pre></div>
</div>
<p>The difference between $W$ and $W_{dq}$ shows that quantization affects precision and appropriate values of scale and zero point will reduce the loss of precision.</p>
</section>
<section id="per-channel-example">
<h4>Per-channel example<a class="headerlink" href="#per-channel-example" title="Permalink to this heading"></a></h4>
<p>Similarly, the example of per-channel quantization is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quantize_per_channel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    <span class="n">q_min</span><span class="p">,</span> <span class="n">q_max</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">2.</span> <span class="o">**</span> <span class="n">num_bits</span> <span class="o">-</span> <span class="mf">1.</span>
    <span class="n">x_tmp</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scales</span> <span class="o">=</span> <span class="n">x_tmp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">num_bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">bias</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">0</span> <span class="o">-</span> <span class="n">x_tmp</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">scales</span><span class="p">))</span>
    <span class="n">q_x</span> <span class="o">=</span> <span class="n">x_tmp</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>
    <span class="n">q_x</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">q_min</span><span class="p">,</span> <span class="n">q_max</span><span class="p">)</span><span class="o">.</span><span class="n">round_</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;scale = </span><span class="si">{</span><span class="n">scales</span><span class="si">}</span><span class="s1">, </span><span class="se">\n</span><span class="s1">bias = </span><span class="si">{</span><span class="n">bias</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q_x</span>

<span class="k">def</span> <span class="nf">dequantize_per_channel</span><span class="p">(</span><span class="n">q_x</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">q_x</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">scales</span> <span class="o">*</span> <span class="p">(</span><span class="n">q_x</span> <span class="o">-</span> <span class="n">bias</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">scales</span> <span class="o">*</span> <span class="p">(</span><span class="n">q_x</span> <span class="o">-</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;W_q<span class="w"> </span><span class="o">=</span><span class="w"> </span>quantize_per_channel<span class="o">(</span>W<span class="o">)</span>
<span class="nv">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>tensor<span class="o">([[</span><span class="m">0</span>.0029<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">0</span>.0036<span class="o">]])</span>,<span class="w"> </span>
<span class="nv">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>tensor<span class="o">([[</span>-162.<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="w"> </span>-48.<span class="o">]])</span>
&gt;&gt;&gt;W_q
tensor<span class="o">([[</span><span class="w"> </span><span class="m">72</span>.,<span class="w">   </span><span class="m">0</span>.,<span class="w">  </span><span class="m">93</span>.<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">207</span>.,<span class="w">   </span><span class="m">0</span>.,<span class="w"> </span><span class="m">139</span>.<span class="o">]])</span>

&gt;&gt;&gt;scales<span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.tensor<span class="o">([[</span><span class="m">0</span>.0027<span class="o">]</span>,<span class="o">[</span><span class="m">0</span>.0017<span class="o">]])</span>
&gt;&gt;&gt;bias<span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.tensor<span class="o">([[</span>-66.<span class="o">]</span>,<span class="o">[</span>-87.<span class="o">]])</span>
&gt;&gt;&gt;W_dq<span class="w"> </span><span class="o">=</span><span class="w"> </span>dequantize_per_channel<span class="o">(</span>W_q,<span class="w"> </span>scales,<span class="w"> </span>bias<span class="o">)</span>
&gt;&gt;&gt;W_dq
tensor<span class="o">([[</span><span class="m">0</span>.6837,<span class="w"> </span><span class="m">0</span>.4734,<span class="w"> </span><span class="m">0</span>.7451<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">0</span>.9301,<span class="w"> </span><span class="m">0</span>.1751,<span class="w"> </span><span class="m">0</span>.6821<span class="o">]])</span>
</pre></div>
</div>
<p>And the loss is</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span><span class="nv">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.nn.MSELoss<span class="o">()(</span>W_dq,<span class="w"> </span>W<span class="o">)</span>
&gt;&gt;&gt;<span class="w"> </span>loss.item<span class="o">()</span>
<span class="m">5</span>.637690492221736e-07
</pre></div>
</div>
<p>Through this example, we can see that per-channel quantization has finer granularity and has lower loss.</p>
</section>
<section id="matmul-quantization-example">
<h4>Matmul quantization example<a class="headerlink" href="#matmul-quantization-example" title="Permalink to this heading"></a></h4>
<p>For a linear layer in most model, $Y=X \cdot W$, we can quantize both the weights and activations in order to reduce the storage and accelerate inference.
Using per-tensor scale quantization to show the process.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quantize_per_tensor_absmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    <span class="n">scales</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">q_max</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">n_bits</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>
    <span class="n">scales</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">q_max</span><span class="p">)</span>
    <span class="n">x</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span><span class="o">.</span><span class="n">round_</span><span class="p">()</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">dequantize</span><span class="p">(</span><span class="n">q_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">q_x</span>
</pre></div>
</div>
<p>Random initialize the $W$ and $Y$, then calculate the result of $Y=X \cdot W$</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;W<span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.rand<span class="o">(</span><span class="m">2</span>,<span class="w"> </span><span class="m">3</span>,<span class="w"> </span><span class="nv">dtype</span><span class="o">=</span>torch.float32<span class="o">)</span>
&gt;&gt;&gt;X<span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.rand<span class="o">(</span><span class="m">3</span>,<span class="w"> </span><span class="m">4</span>,<span class="w"> </span><span class="nv">dtype</span><span class="o">=</span>torch.float32<span class="o">)</span>
&gt;&gt;&gt;W
tensor<span class="o">([[</span><span class="m">0</span>.0806,<span class="w"> </span><span class="m">0</span>.7589,<span class="w"> </span><span class="m">0</span>.6038<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">0</span>.3815,<span class="w"> </span><span class="m">0</span>.5040,<span class="w"> </span><span class="m">0</span>.7174<span class="o">]])</span>
&gt;&gt;&gt;X
tensor<span class="o">([[</span><span class="m">0</span>.5444,<span class="w"> </span><span class="m">0</span>.5826,<span class="w"> </span><span class="m">0</span>.7772,<span class="w"> </span><span class="m">0</span>.5555<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">0</span>.3740,<span class="w"> </span><span class="m">0</span>.3253,<span class="w"> </span><span class="m">0</span>.0698,<span class="w"> </span><span class="m">0</span>.1381<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">0</span>.5972,<span class="w"> </span><span class="m">0</span>.0086,<span class="w"> </span><span class="m">0</span>.0737,<span class="w"> </span><span class="m">0</span>.8298<span class="o">]])</span>
&gt;&gt;&gt;Y<span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.matmul<span class="o">(</span>W,<span class="w"> </span>X<span class="o">)</span>
&gt;&gt;&gt;Y
tensor<span class="o">([[</span><span class="m">0</span>.6883,<span class="w"> </span><span class="m">0</span>.2991,<span class="w"> </span><span class="m">0</span>.1601,<span class="w"> </span><span class="m">0</span>.6506<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">0</span>.8246,<span class="w"> </span><span class="m">0</span>.3924,<span class="w"> </span><span class="m">0</span>.3845,<span class="w"> </span><span class="m">0</span>.8768<span class="o">]])</span>
</pre></div>
</div>
<p>Quantize weight and activation, matmul(quantize(X), quantize(Y))</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;W_q,<span class="w"> </span><span class="nv">W_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>quantize_per_tensor_absmax<span class="o">(</span>W<span class="o">)</span>
&gt;&gt;&gt;X_q,<span class="w"> </span><span class="nv">X_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>quantize_per_tensor_absmax<span class="o">(</span>X<span class="o">)</span>
&gt;&gt;&gt;print<span class="o">(</span>f<span class="s1">&#39;{W_q}\n{W_scale.item()}&#39;</span><span class="o">)</span>
&gt;&gt;&gt;print<span class="o">(</span>f<span class="s1">&#39;{X_q}\n{X_scale.item()}&#39;</span><span class="o">)</span>
tensor<span class="o">([[</span><span class="w"> </span><span class="m">13</span>.,<span class="w"> </span><span class="m">127</span>.,<span class="w"> </span><span class="m">101</span>.<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="w"> </span><span class="m">64</span>.,<span class="w">  </span><span class="m">84</span>.,<span class="w"> </span><span class="m">120</span>.<span class="o">]])</span>
<span class="m">0</span>.0059755356051027775
tensor<span class="o">([[</span><span class="w"> </span><span class="m">83</span>.,<span class="w">  </span><span class="m">89</span>.,<span class="w"> </span><span class="m">119</span>.,<span class="w">  </span><span class="m">85</span>.<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="w"> </span><span class="m">57</span>.,<span class="w">  </span><span class="m">50</span>.,<span class="w">  </span><span class="m">11</span>.,<span class="w">  </span><span class="m">21</span>.<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="w"> </span><span class="m">91</span>.,<span class="w">   </span><span class="m">1</span>.,<span class="w">  </span><span class="m">11</span>.,<span class="w"> </span><span class="m">127</span>.<span class="o">]])</span>
<span class="m">0</span>.006533813662827015

&gt;&gt;&gt;Y_q<span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.matmul<span class="o">(</span>W_q,<span class="w"> </span>X_q<span class="o">)</span>
&gt;&gt;&gt;Y_q
tensor<span class="o">([[</span><span class="m">17509</span>.,<span class="w">  </span><span class="m">7608</span>.,<span class="w">  </span><span class="m">4055</span>.,<span class="w"> </span><span class="m">16599</span>.<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">21020</span>.,<span class="w"> </span><span class="m">10016</span>.,<span class="w">  </span><span class="m">9860</span>.,<span class="w"> </span><span class="m">22444</span>.<span class="o">]])</span>
&gt;&gt;&gt;Y_dq<span class="w"> </span><span class="o">=</span><span class="w"> </span>dequantize<span class="o">(</span>Y,<span class="w"> </span>W_scale<span class="w"> </span>*<span class="w"> </span>X_scale<span class="o">)</span>
&gt;&gt;&gt;Y_dq
tensor<span class="o">([[</span><span class="m">0</span>.6836,<span class="w"> </span><span class="m">0</span>.2970,<span class="w"> </span><span class="m">0</span>.1583,<span class="w"> </span><span class="m">0</span>.6481<span class="o">]</span>,
<span class="w">        </span><span class="o">[</span><span class="m">0</span>.8207,<span class="w"> </span><span class="m">0</span>.3911,<span class="w"> </span><span class="m">0</span>.3850,<span class="w"> </span><span class="m">0</span>.8763<span class="o">]])</span>
</pre></div>
</div>
</section>
<section id="per-channel-limitation">
<h4>Per-channel limitation<a class="headerlink" href="#per-channel-limitation" title="Permalink to this heading"></a></h4>
<p>Though per-channel quantization could bring lower quantization error, we could not apply it for activations due to the difficulty of the dequantization. We would prove it in the following image and the zero point of quantization would be ignored for simplicity.</p>
<p>The left side of the image presents a normal linear forward  with 1x2 input $x$ and 2x2 weight $w$. The results $y$ could be easily obtained by simple mathematics. In the middle sub-image, we apply per-tensor quantization for activations and per-channel quantization for weights; the results after quantization that are denoted by $y_1$ and $y_2$, could be easily dequantized to the float results $y_{fp1}$ and $y_{fp2}$ by per channel scale $1.0/s_1s_x$ and $1.0/s_2s_x$. However, after applying per-channel quantization for activation on the right sub-image, we could not dequantize the  $y_1$ and  $y_2$ to float results.</p>
<div align="center">
    <img src="./imgs/sq_pc.png"/>
</div></section>
</section>
</section>
<section id="smoothquant-and-our-enhancement">
<h2>SmoothQuant and Our Enhancement<a class="headerlink" href="#smoothquant-and-our-enhancement" title="Permalink to this heading"></a></h2>
<section id="smoothquant">
<h3>SmoothQuant<a class="headerlink" href="#smoothquant" title="Permalink to this heading"></a></h3>
<p>In the previous subsection, we have explained why per-channel quantization could not be applied for activation, even though it could lead to lower quantization loss. However, the quantization error loss of activation plays an important role in the accuracy loss of model quantization[^2][^3][^4].</p>
<p>To reduce the quantization loss of activations, lots of methods have been proposed. In the following, we briefly introduce SPIQ[^2], Outlier Suppression[^3] and Smoothquant[^4]. All these three methods share a similar idea to migrate the difficulty from activation quantization to weight quantization but differ in how much difficulty to be transferred.</p>
<p>So <strong>the first question is how to migrate the difficulty from activation to weights?</strong> The solution is straightforward, that is to convert the network to an output equivalent network that is presented in the image below and apply quantization to this equivalent network. The intuition is that each channel of activation could be scaled to make it more quantization-friendly, similar to a fake per-channel activation quantization.</p>
<div align="center">
    <img src="./imgs/sq_convert.png"/>
</div><p>Please note that this conversion will make the quantization of weights more difficult, because the scales attached to weights showed above are per-input-channel, while quantization of weights is per-output-channel or per-tensor.</p>
<p>So <strong>the second question is how much difficulty to be migrated</strong>, that is how to choose the <strong>convention per-channel scale</strong> $s_{x1}$ and $s_{x2}$ on the above image. Different works adopt different ways.</p>
<p><em>SPIQ</em> just adopts the quantization scale of activations as the convention per-channel scale.</p>
<p><em>Outlier suppression</em> adopts the scale of the preceding layernorm as the convention per-channel scale.</p>
<p><em>Smoothquant</em> introduces a hyperparameter $\alpha$ as a smooth factor to calculate the convention per-channel scale and balance the quantization difficulty of activation and weight.</p>
<p>$$
s_j = max(|X_j|)^\alpha/max(|W_j|)^{1-\alpha} \tag{4}
$$</p>
<p>j is the index of the input channels.</p>
<div align="center">
    <img src="./imgs/smoothquant.png" height="250"/>
</div><p>For most of the models such as OPT and BLOOM, $\alpha = 0.5$ is a well-balanced value to split the difficulty of weight and activation quantization. A larger $\alpha$ value could be used on models with more significant activation outliers to migrate more quantization difficulty to weights.</p>
</section>
<section id="our-enhancement">
<h3>Our enhancement:<a class="headerlink" href="#our-enhancement" title="Permalink to this heading"></a></h3>
<section id="algorithm-layer-wise-auto-tuning-of-alpha">
<h4>Algorithm: Layer-wise Auto-tuning of $\alpha$.<a class="headerlink" href="#algorithm-layer-wise-auto-tuning-of-alpha" title="Permalink to this heading"></a></h4>
<p>SmoothQuant method aims to split the quantization difficulty of weight and activation by using a fixed-value $\alpha$ for an entire model. However, as the distributions of activation outliers vary not only across different models but also across different layers within a model, we hereby propose a method to obtain layer-wise optimal $\alpha$ values with the ability to tune automatically.</p>
<p>Our proposed method consists of 5 major steps:</p>
<ul class="simple">
<li><p>Hook input and output values of all layers using register_forward_hook.</p></li>
<li><p>Generate a list of $\alpha$ values given user-defined $\alpha$ range and step_sizes.</p></li>
<li><p>Recalculate smoothing factor given an $\alpha$ value and adjust parameters(weights and activations).</p></li>
<li><p>Perform per-channel quantization_dequantization of weights and per-tensor quantization_dequantization of inputs to predict the layer-wise output corresponding to the given $\alpha$ value.</p></li>
<li><p>Calculate the mean-squared loss with respect to the actual output value, recover the adjusted parameters and save the layer-wise optimal $\alpha$ values.</p></li>
</ul>
<p>Multiple criteria (e.g min, max and mean) are supported to determine the $\alpha$ value of an input LayerNorm op of a transformer block.</p>
<p>In our experiments, an $\alpha$ range of [0.3, 0.7] with a step_size of 0.05 is found to be well-balanced one for the majority of models.</p>
</section>
<section id="engineering">
<h4>Engineering<a class="headerlink" href="#engineering" title="Permalink to this heading"></a></h4>
<p><em>fully automated</em>: the user only needs to pass a model and dataloader</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.adaptor.torch_utils.smooth_quant</span> <span class="kn">import</span> <span class="n">TorchSmoothQuant</span>
<span class="n">sq</span> <span class="o">=</span> <span class="n">TorchSmoothQuant</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>
<span class="n">sq</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="c1">##alpha could be a float or a string ’auto‘</span>
</pre></div>
</div>
<p>please note that we rely on torch jit to analyze the model. If you are using huggingface model, you could set torchscript to True when loading the model or set the return_dict to False”</p>
<p><em>support lots of fusing patterns</em>: the official code only supports</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>linear-&gt;layernorm
</pre></div>
</div>
<p>while we support the following patterns</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conv2d/linear-&gt;relu/leakyrelu/hardtanh-&gt;conv2d/linear/layernorm/batchnorm/instancenorm/t5norm/llamanorm/groupnorm/

conv2d/linear-&gt;conv2d/linear/layernorm/batchnorm/instancenorm/t5norm/llamanorm/groupnorm
</pre></div>
</div>
<p>For opt models, we could fuse one more layer than the official code, because the fc2 layer in the block follows the linear-&gt;relu-&gt;linear pattern.</p>
</section>
</section>
</section>
<section id="validated-models">
<h2>Validated Models<a class="headerlink" href="#validated-models" title="Permalink to this heading"></a></h2>
<p>| Model\Last token accuracy |  FP32  | INT8 (w/o SmoothQuant) | INT8 (w/ SmoothQuant) | INT8 (w/ SmoothQuant auto tuning) |
|———————|:——:|:———————-:|———————–|———————————–|
| bigscience/bloom-560m | 65.20% |         63.44%         | 66.48% (alpha=0.5)    | 64.76%                            |
| bigscience/bloom-1b7 | 71.43% |         67.78%         | 72.56% (alpha=0.5)    | 72.58%                            |
| bigscience/bloom-3b | 73.97% |         69.99%         | 74.02% (alpha=0.5)    | 74.16%                            |
| bigscience/bloom-7b1 | 77.44% |         75.46%         | 77.02%(alpha=0.5)     | 77.45%                            |
| bigscience/bloom-176b | 84.17% |         82.13%         | 83.52% (alpha=0.6)    | -                                 |
| facebook/opt-125m   | 63.89% |         63.48%         | 63.44% (alpha=0.5)    | 64.14%                            |
| facebook/opt-1.3b   | 75.41% |         73.59%         | 70.94% (alpha=0.5)    | 74.80%                            |
| facebook/opt-2.7b   | 77.79% |         78.57%         | 78.60%(alpha=0.5)     | 78.25%                            |
| facebook/opt-6.7b   | 81.26% |         76.65%         | 81.58%(alpha=0.5)     | 81.39%                            |
| EleutherAI/gpt-j-6B | 79.17% |         78.82%         | 78.84%(alpha=0.6)     | 79.29%                            |</p>
</section>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this heading"></a></h2>
<p>User could refer to <a class="reference external" href="https://github.com/intel/neural-compressor/blob/master/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/ptq_static/ipex/smooth_quant/README.md">examples</a> on how to use smooth quant.</p>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this heading"></a></h2>
<p>[^1]: Jason, Wei, et al. “Emergent Abilities of Large Language Models”. Published in Transactions on Machine Learning Research (2022)</p>
<p>[^2]: Yvinec, Edouard, et al. “SPIQ: Data-Free Per-Channel Static Input Quantization.” Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023.</p>
<p>[^3]: Wei, Xiuying, et al. “Outlier suppression: Pushing the limit of low-bit transformer language models.” arXiv preprint arXiv:2209.13325 (2022).</p>
<p>[^4]: Xiao, Guangxuan, et al. “Smoothquant: Accurate and efficient post-training quantization for large language models.” arXiv preprint arXiv:2211.10438 (2022)..</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>