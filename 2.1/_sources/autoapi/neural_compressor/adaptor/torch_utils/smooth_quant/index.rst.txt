:orphan:

:py:mod:`neural_compressor.adaptor.torch_utils.smooth_quant`
============================================================

.. py:module:: neural_compressor.adaptor.torch_utils.smooth_quant


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   neural_compressor.adaptor.torch_utils.smooth_quant.TorchSmoothQuant
   neural_compressor.adaptor.torch_utils.smooth_quant.GraphTrace




.. py:class:: TorchSmoothQuant(model, dataloader, traced_model=None)


   Fake input channel quantization, for more details please refer to
   [1] SmoothQuant: Accurate and Efficient
   Post-Training Quantization for Large Language Models
   [2] SPIQ: Data-Free Per-Channel Static Input Quantization
   Currently, we only handle the layers whose smooth scale could be absorbed, we will support other layers later.
   We only support inplace mode which means the model weights will be changed, you can call recover function only
   once to recover the weights if needed


.. py:class:: GraphTrace


   


